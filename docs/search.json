[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harmon’s public notebook",
    "section": "",
    "text": "Workflow of Mengyi et al. (2023)\n\n\nPooled plasma from China (DNA)\n\n\n\n\n\n\n\n\nJul 23, 2024\n\n\nHarmon Bhasin\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow of Thijssen et al. (2023)\n\n\nPooled plasma from Iran (DNA + RNA)\n\n\n\n\n\n\n\n\nJul 22, 2024\n\n\nHarmon Bhasin\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow of Blauwkamp et al. (2019)\n\n\nIndividual plasma from USA\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nHarmon Bhasin\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow of Cebria-Mendoza et al. (2021)\n\n\nPooled plasma from Spain (DNA + RNA)\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\nHarmon Bhasin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html",
    "href": "notebooks/2024-07-08_cebria-mendoza.html",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "",
    "text": "As a part of my time here, I’m exploring blood-based surveillance for novel viral pathogen detection. We’ve decided to make three blog posts. The first blog post will cover the physical and biological characteristics of blood, the second blog post will cover the blood sampling strategies, and the last blog post will look at the metagenomic profile of blood (which is what we’re doing here, and will do for the next few posts).\nThis is the first of many studies that I’ll be analyzing. however I don’t expect all of them to be used in the last blog post. In this post, I analyze Cebria-Mendoza 2021, a dataset with 60 samples where each sample is a pool of 8-13 unique healthy donors, with a total of ~600 donors from Spain."
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#high-level-metrics",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#high-level-metrics",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "High-level metrics",
    "text": "High-level metrics\nThe average fraction of reads at each stage in the preprocessing pipeline is shown in the following table. On average, cleaning & deduplication removed about 57% of total read pairs, primarily during duplication which makes sense given that the raw metrics show that the samples are quite high in duplication. Ribodepletion removed about 6-8% during each round.\n\nCode# TODO: Group by pool size as well\n# Count read losses\nn_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, pool_size, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample, pool_size) %&gt;% \n  arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\nAdapter content staays low. Read quality stays pretty similar over the positions, but improves over the number of sequences (predominantly from cleaning) throughout the pipeline.\n\nCodeg_qual &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,1),\n                     , expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~., scales = \"free_y\")\ng_quality_seq\n\n\n\n\n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, group=sample)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, \n                      aes(x=stage, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nDuplication rates are still quite high which is a bit concerning. Mean read length stays close to expected 150.\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nSome samples had anywhere as low as 10% all the way up to 60% ribosomal reads.\n\nCode# Calculate reads lost during ribodepletion (approximation for % ribosomal reads)\nreads_ribo &lt;- n_reads_rel %&gt;% \n  filter(stage %in% c(\"dedup\", \"ribo_secondary\")) %&gt;% \n  group_by(sample) %&gt;% \n  summarize(p_reads_ribo=1-n_read_pairs[2]/n_read_pairs[1], .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\nreads_ribo_summ &lt;- reads_ribo %&gt;%\n  group_by(sample) %&gt;%\n  summarize(min=min(p_reads_ribo), max=max(p_reads_ribo),\n            mean=mean(p_reads_ribo), .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\ng_reads_ribo &lt;- ggplot(reads_ribo, \n                       aes(x=library, y=p_reads_ribo)) +\n  geom_point() + \n  scale_y_continuous(name=\"Approx % ribosomal reads\", limits=c(0,1),\n                     breaks=seq(0,1,0.2), expand=c(0,0), labels = function(y) y*100)+\n  theme_kit\ng_reads_ribo"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#effectiveness-of-ribodepletion",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#effectiveness-of-ribodepletion",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Effectiveness of ribodepletion",
    "text": "Effectiveness of ribodepletion\nSome samples had anywhere as low as 10% all the way up to 60% ribosomal reads.\n\nCode# Calculate reads lost during ribodepletion (approximation for % ribosomal reads)\nreads_ribo &lt;- n_reads_rel %&gt;% \n  filter(stage %in% c(\"dedup\", \"ribo_secondary\")) %&gt;% \n  group_by(sample) %&gt;% \n  summarize(p_reads_ribo=1-n_read_pairs[2]/n_read_pairs[1], .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\nreads_ribo_summ &lt;- reads_ribo %&gt;%\n  group_by(sample) %&gt;%\n  summarize(min=min(p_reads_ribo), max=max(p_reads_ribo),\n            mean=mean(p_reads_ribo), .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\ng_reads_ribo &lt;- ggplot(reads_ribo, \n                       aes(x=library, y=p_reads_ribo)) +\n  geom_point() + \n  scale_y_continuous(name=\"Approx % ribosomal reads\", limits=c(0,1),\n                     breaks=seq(0,1,0.2), expand=c(0,0), labels = function(y) y*100)+\n  theme_kit\ng_reads_ribo"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#high-level-composition",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#high-level-composition",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "High-level composition",
    "text": "High-level composition\nTo assess the high-level composition of the reads, we ran them through Kraken2 and summarized the results with Bracken.\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=library, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Plasma pool\") +\n  theme_kit + \n  theme(plot.title = element_text(hjust=0, face=\"plain\", size=rel(1.5)))\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\ngeom_comp &lt;- purrr::partial(geom_col, position = \"stack\", width = 1)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_comp(data = comp) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, all groups)\")\ng_comp\n\n\n\n\n\n\nCode# Repeat for classified reads only\npalette_assigned &lt;- brewer.pal(9, \"Set1\")[5:9]\ng_comp_assigned &lt;- g_comp_base + \n  geom_comp(data = comp_assigned) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_manual(values=palette_assigned, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, all groups)\")\ng_comp_assigned\n\n\n\n\n\n\nCode# Plot composition of minor components\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_comp(data=comp_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, minor groups)\")\ng_comp_minor\n\n\n\n\n\n\nCodeg_comp_assigned_minor &lt;- g_comp_base + \n  geom_comp(data=comp_assigned_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, minor groups)\")\ng_comp_assigned_minor"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#total-viral-content",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#total-viral-content",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Total viral content",
    "text": "Total viral content\nTotal viral fraction average \\(1.81 \\times 10^{-2}\\) across samples. As a fraction of assigned (rather than total) reads, this jumped to \\(1.60 \\times {-1}\\):\n\nCodep_reads_viral_all &lt;- comp %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"All reads\")\np_reads_viral_assigned &lt;- comp_assigned %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"Classified reads\")\np_reads_viral &lt;- bind_rows(p_reads_viral_all, p_reads_viral_assigned)\n\n# Plot\ng_viral &lt;- ggplot(p_reads_viral, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_x_discrete(name=\"Plasma pool\") +\n  scale_y_log10(name=\"Viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  guides(color=guide_legend(nrow=2), shape=guide_legend(nrow=2),\n         linetype=guide_legend(nrow=2)) +\n  theme_kit\ng_viral"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#taxonomic-composition-of-viruses",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#taxonomic-composition-of-viruses",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Taxonomic composition of viruses",
    "text": "Taxonomic composition of viruses\nThe two dominant viruses we see are Anellovirdae and Rhabdovirdae (spike-in). Followed by these two viral families is Flavivirdae, and lastly, also by a much smaller percent, Microviridae (spike-in). The threshold for the label “other” are the set of families that make up less than 1% composition in all samples.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"total-virus-db.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get Kraken reports\nreports_path &lt;- file.path(tax_final_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% arrange(sample)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %&gt;% ungroup\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.01\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries, by='sample')\n\n# Plot\ng_families &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_brewer(palette = 'Accent')\ng_families\n\n\n\n\n\n\n\nExcluding Anellovirdae and Rhabdovirdae, remaining viral sequences are distributed across a wide variety:\n\nCodemajor_threshold_adj &lt;- 0.05\n\n# Adjust viral family counts\nviral_families_adj &lt;- viral_families %&gt;%\n  filter(!(name %in% c(\"Rhabdoviridae\",\"Anelloviridae\"))) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = p_reads_viral/sum(p_reads_viral))\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families_adj %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families_adj %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries, by = 'sample')\n\n# Plot\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"), brewer.pal(9, \"Set1\"))\ng_families_adj &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\ng_families_adj"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#overall-relative-abundance",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#overall-relative-abundance",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Overall relative abundance",
    "text": "Overall relative abundance\nI calculated the relative abundance of human-infecting viruses in two ways:\n\nFirst, as the total number of deduplicated human-virus reads in each sample, divided by the number of raw reads (“All reads”).\nSecond, as a fraction of preprocessed (cleaned, deduplicated, computationally ribodepleted) reads (“Preprocessed reads”).\n\n[Analyze]\n\nCode# Get raw read counts\nread_counts_raw &lt;- filter(basic_stats_raw) %&gt;%\n  select(sample, n_reads_raw = n_read_pairs)\nread_counts_preproc &lt;- basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;%\n  select(sample, n_reads_preproc = n_read_pairs)\n\n# Get HV read counts\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% \n  group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=c(\"sample\")) %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0)) %&gt;%\n  left_join(read_counts_preproc, by=c(\"sample\")) %&gt;%\n  inner_join(libraries, by=c(\"sample\")) %&gt;%\n  select(sample, n_reads_raw, n_reads_preproc, n_reads_hv) %&gt;%\n  mutate(n_samples = 1,\n         p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc)\nread_counts_long &lt;- read_counts %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\"))\n\n# Combine for display\nread_counts_agg &lt;- read_counts %&gt;%\n  mutate(p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc) %&gt;%\n  inner_join(libraries, by=c(\"sample\"))\nread_counts_agg_long &lt;- read_counts_agg %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\")) \n\n# Visualize\ng_read_counts &lt;- ggplot(read_counts_agg_long, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_y_log10(name = \"Unique human-viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  theme_kit\ng_read_counts"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#overall-taxonomy-and-composition",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#overall-taxonomy-and-composition",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Overall taxonomy and composition",
    "text": "Overall taxonomy and composition\nComposition of HV reads was not greatly changed from when looking at all viral reads. The two dominant viruses we see are Anellovirdae and Rhabdovirdae. Followed by these two viral families is Flavivirdae, and lastly, also by a much smaller percent, Microviridae. The threshold for the label “other” are the set of families that make up less than 5% composition in all samples.\n\nCodethreshold_major_family &lt;- 0.05\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display) %&gt;%\n  inner_join(libraries, by = 'sample')\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_brewer(palette = 'Accent', name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;%\n  group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv),\n            p_reads_max = max(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\n#hv_family_collate\n\n\n\nCodethreshold_major_family &lt;- 0.05\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  filter(!(name %in% c(\"Anelloviridae\", \"Rhabdoviridae\"))) %&gt;%\n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display) %&gt;%\n  inner_join(libraries, by = 'sample')\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_brewer(palette = 'Accent', name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;%\n  group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv),\n            p_reads_max = max(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\nhv_family_collate"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html",
    "href": "notebooks/2024-07-09_blauwkamp.html",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "",
    "text": "THIS IS CURRENTLY A WORK IN PROGRESS!\nThis is the second study of this series. In this post, I analyze Blauwkamp 2019, a dataset with ~170 samples, one for each individual of cell-free DNA in plasma in the United States."
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#high-level-metrics",
    "href": "notebooks/2024-07-09_blauwkamp.html#high-level-metrics",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "High-level metrics",
    "text": "High-level metrics\nThe average fraction of reads at each stage in the preprocessing pipeline is shown in the following table. Given that human reads were removed before hand, we’d expect our trimming and filtering to make minimal change, and that’s what we see. However, the researchers did not deduplicate and we can see that that removes a large portion of our reads, about 25% on average. Ribdepletion doesn’t remove any reads, I’m not sure how to feel about this.\n\nCode# Count read losses\nn_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% \n  arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, group=sample)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Reads\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, \n                      aes(x=stage, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nTODO interpret the below plot.\n\nCodeg_qual &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read\") +\n  theme_base\n\n# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~., scales = \"free_y\")\ng_quality_seq\n\n\n\n\n\n\n\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#effectiveness-of-ribodepletion",
    "href": "notebooks/2024-07-09_blauwkamp.html#effectiveness-of-ribodepletion",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "Effectiveness of ribodepletion",
    "text": "Effectiveness of ribodepletion\n\nCode# Calculate reads lost during ribodepletion (approximation for % ribosomal reads)\nreads_ribo &lt;- n_reads_rel %&gt;% \n  filter(stage %in% c(\"dedup\", \"ribo_secondary\")) %&gt;% \n  group_by(sample) %&gt;% \n  summarize(p_reads_ribo=1-n_read_pairs[2]/n_read_pairs[1], .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\nreads_ribo_summ &lt;- reads_ribo %&gt;%\n  group_by(sample) %&gt;%\n  summarize(min=min(p_reads_ribo), max=max(p_reads_ribo),\n            mean=mean(p_reads_ribo), .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\ng_reads_ribo &lt;- ggplot(reads_ribo, \n                       aes(x=library, y=p_reads_ribo)) +\n  geom_point() + \n  scale_y_continuous(name=\"Approx % ribosomal reads\", limits=c(0,1),\n                     breaks=seq(0,1,0.2), expand=c(0,0), labels = function(y) y*100)+\n  theme_kit\ng_reads_ribo"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#high-level-composition",
    "href": "notebooks/2024-07-09_blauwkamp.html#high-level-composition",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "High-level composition",
    "text": "High-level composition\nTo assess the high-level composition of the reads, I ran the ribodepleted files through Kraken2 and summarized the results with Bracken.\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=library, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Plasma pool\") +\n  theme_kit + \n  theme(plot.title = element_text(hjust=0, face=\"plain\", size=rel(1.5)))\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% of reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\ngeom_comp &lt;- purrr::partial(geom_col, position = \"stack\", width = 1)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_comp(data = comp) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, all groups)\")\ng_comp\n\n\n\n\n\n\nCode# Repeat for classified reads only\npalette_assigned &lt;- brewer.pal(9, \"Set1\")[5:9]\ng_comp_assigned &lt;- g_comp_base + \n  geom_comp(data = comp_assigned) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_manual(values=palette_assigned, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, all groups)\")\ng_comp_assigned\n\n\n\n\n\n\nCode# Plot composition of minor components\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_comp(data=comp_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, minor groups)\")\ng_comp_minor\n\n\n\n\n\n\nCodeg_comp_assigned_minor &lt;- g_comp_base + \n  geom_comp(data=comp_assigned_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, minor groups)\")\ng_comp_assigned_minor\n\n\n\n\n\n\n\nTODO analyze this above"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#total-viral-content",
    "href": "notebooks/2024-07-09_blauwkamp.html#total-viral-content",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "Total viral content",
    "text": "Total viral content\nTotal viral fraction average \\(6.13 \\times 10^{-6}\\) across samples. As a fraction of assigned (rather than total) reads, this jumped to \\(1.88 \\times 10^{-3}\\):\n\nCodep_reads_viral_all &lt;- comp %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"All reads\")\np_reads_viral_assigned &lt;- comp_assigned %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"Classified reads\")\np_reads_viral &lt;- bind_rows(p_reads_viral_all, p_reads_viral_assigned)\n\n# Plot\ng_viral &lt;- ggplot(p_reads_viral, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_x_discrete(name=\"Plasma pool\") +\n  scale_y_log10(name=\"Viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  guides(color=guide_legend(nrow=2), shape=guide_legend(nrow=2),\n         linetype=guide_legend(nrow=2)) +\n  theme_kit\ng_viral"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#taxonomic-composition-of-viruses",
    "href": "notebooks/2024-07-09_blauwkamp.html#taxonomic-composition-of-viruses",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "Taxonomic composition of viruses",
    "text": "Taxonomic composition of viruses\nWhen we drop down to the taxonomic composition of viral families, we go from 167 samples to 119 samples.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"total-virus-db.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get Kraken reports\nreports_path &lt;- file.path(tax_final_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% arrange(sample)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %&gt;% ungroup\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.01\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries, by='sample')\n\n# Plot\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_families &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\ng_families\n\n\n\n\n\n\n\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"total-virus-db.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get Kraken reports\nreports_path &lt;- file.path(tax_final_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% arrange(sample) %&gt;% drop_na() \n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %&gt;% \n  rename(old_rank=rank) %&gt;% ungroup\n\nmajor_threshold &lt;- 0.01\n\n# Identify major viral families\nall_virus_name &lt;- kraken_reports_viral_cleaned %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold) %&gt;%\n  pull(name)\n\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral_cleaned %&gt;% \n  filter(name %in% all_virus_name) %&gt;%\n  select(name, taxid, sample, p_reads_viral, n_reads_clade)\n\n\n\nCode#ranks_of_interest &lt;- rev(c(\"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\"))\nranks_of_interest &lt;- c(\"family\", \"genus\")\n\n# Filter the ranks we're interested in\nfiltered_taxa &lt;- viral_taxa %&gt;%\n  filter(rank %in% ranks_of_interest)\n\n# Join the datasets and create the links dataframe\nlinks &lt;- kraken_reports_viral_cleaned %&gt;%\n  inner_join(filtered_taxa, by = c(\"taxid\", \"name\")) %&gt;%\n  inner_join(filtered_taxa, by = c(\"parent_taxid\" = \"taxid\"), suffix = c(\"\", \"_parent\")) %&gt;%\n  #filter(rank != rank_parent) %&gt;%  # Ensure source and target are not in the same rank\n  select(source = name_parent, target = name, value = p_reads_viral)\n\n# Create nodes dataframe from unique names in links\nnodes &lt;- data.frame(\n  name = c(links$source, links$target) %&gt;% unique()\n) %&gt;%\n  mutate(node = row_number() - 1)  # zero-indexed for networkD3\n\n# Join rank information to nodes\nnodes &lt;- nodes %&gt;%\n  left_join(filtered_taxa, by = c(\"name\")) %&gt;%\n  select(name, node, rank)\n\n# Add IDs to links dataframe\nlinks &lt;- links %&gt;%\n  left_join(nodes, by = c(\"source\" = \"name\")) %&gt;%\n  rename(IDsource = node) %&gt;%\n  left_join(nodes, by = c(\"target\" = \"name\")) %&gt;%\n  rename(IDtarget = node)\n\n# Aggregate link values\nlinks_aggregated &lt;- links %&gt;%\n  group_by(IDsource, IDtarget, source, target) %&gt;%\n  summarise(value = sum(value), .groups = \"drop\")\n\n# Get rid of some warnings\nlinks_aggregated &lt;- as.data.frame(links_aggregated)\nnodes &lt;- as.data.frame(nodes)\n\n# Create Sankey diagram\nsankey_diagram &lt;- sankeyNetwork(Links = links_aggregated, \n                                Nodes = nodes,\n                                Source = \"IDsource\", \n                                Target = \"IDtarget\",\n                                Value = \"value\", \n                                NodeID = \"name\",\n                                fontSize = 12,\n                                NodeGroup = \"rank\",\n                                nodeWidth = 30,\n                                height = 800,\n                                width = 1000,\n                                sinksRight = FALSE)\n\n# Display the diagram\n#sankey_diagram"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#number-of-reads-for-viruses-of-interest",
    "href": "notebooks/2024-07-09_blauwkamp.html#number-of-reads-for-viruses-of-interest",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "Number of reads for viruses of interest",
    "text": "Number of reads for viruses of interest\nWe’re particulary interested in the following viruses:\n\nHepatitis B\n\nFamily: Hepadnaviridae\nGenus: Orthohepadnavirus\nSpecies: Hepatitis B virus\n\n\nHepatitis C\n\nFamily: Flaviviridae\nGenus: Hepacivirus\nSpecies: Hepatitis C virus\n\n\nEBV (Epstein-Barr virus)\n\nFamily: Herpesviridae\nGenus: Lymphocryptovirus\nSpecies: Human gammaherpesvirus 4\n\n\nCMV (Cytomegalovirus)\n\nFamily: Herpesviridae\nGenus: Cytomegalovirus\nSpecies: Human betaherpesvirus 5\n\n\nHIV-1 (Human Immunodeficiency Virus type 1)\n\nFamily: Retroviridae\nGenus: Lentivirus\nSpecies: Human immunodeficiency virus 1\n\n\nAnelloviridae\n\nFamily: Anelloviridae\n\n\n\n\nCode#reads_post_process &lt;- basic_stats %&gt;% filter(stage == 'ribo_secondary') %&gt;% \n  #select(sample, n_read_pairs)\n\nviral_families_interest &lt;- viral_families %&gt;% filter(name == \"Hepadnaviridae\" | \n    name ==  \"Flaviviridae\" | \n    name ==  \"Orthoherpesviridae\" |\n    name ==  \"Herpesviridae\" |\n    name ==  \"Retroviridae\" |\n    name == \"Anelloviridae\") %&gt;% group_by(name) %&gt;% summarize(sum=sum(n_reads_clade))\n\nviral_families_interest\n\n\n  \n\n\n\n\nCode# Species\n# Hepatitis B virus &lt;- 10407\n# Hepatitis C virus &lt;- 10407\n# HIV - 1 &lt;- 11676\n\nviral_families_interest &lt;- viral_genus %&gt;% filter( name == \"Orthohepadnavirus\" | \n    name ==  \"Hepacivirus\" | \n    name ==  \"Lymphocryptovirus\" |\n    name ==  \"Cytomegalovirus\" |\n    name == \"Lentivirus\") %&gt;% group_by(name) %&gt;% summarize(sum=sum(n_reads_clade))\n\nviral_families_interest"
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html",
    "href": "notebooks/2024-07-22-thijssen.html",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "",
    "text": "This is another potential study of this series. In this post, I analyze Thijssen 2023, a dataset with 20 pooled samples from 100 healthy individuals in Iran."
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#high-level-metrics",
    "href": "notebooks/2024-07-22-thijssen.html#high-level-metrics",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "High-level metrics",
    "text": "High-level metrics\nThe average fraction of reads at each stage in the preprocessing pipeline is shown in the following table. Adapter trimming & filtering doesn’t lose too many reads. Deduplication loses us about 30% of reads on average, then ribodepletion only loses as about 0.7% on average.\n\nCode# TODO: Group by pool size as well\n# Count read losses\nn_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% \n  arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\nThese plots below show the trends from above in each sample.\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, group=sample)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, \n                      aes(x=stage, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nHaven’t spent the time to interpret the adapter stuff as yet. Q score remain the same during read cleaning when looking at the positions, with the end of the read actually improving in score. Q scores across all sequences look pretty much the same throughout cleaning.\n\nCodeg_qual &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~., scales = \"free_y\")\ng_quality_seq\n\n\n\n\n\n\n\nAll samples tend to follow similar trends for deduplication, with a large decrease in read length post adapter trimming.\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nRibosomal reads were quite low, near 1% for every sample.\n\nCode# Calculate reads lost during ribodepletion (approximation for % ribosomal reads)\nreads_ribo &lt;- n_reads_rel %&gt;% \n  filter(stage %in% c(\"dedup\", \"ribo_secondary\")) %&gt;% \n  group_by(sample) %&gt;% \n  summarize(p_reads_ribo=1-n_read_pairs[2]/n_read_pairs[1], .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\nreads_ribo_summ &lt;- reads_ribo %&gt;%\n  group_by(sample) %&gt;%\n  summarize(min=min(p_reads_ribo), max=max(p_reads_ribo),\n            mean=mean(p_reads_ribo), .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\ng_reads_ribo &lt;- ggplot(reads_ribo, \n                       aes(x=library, y=p_reads_ribo)) +\n  geom_point() + \n  scale_y_continuous(name=\"Approx % ribosomal reads\", limits=c(0,1),\n                     breaks=seq(0,1,0.2), expand=c(0,0), labels = function(y) y*100)+\n  theme_kit\ng_reads_ribo"
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#high-level-composition",
    "href": "notebooks/2024-07-22-thijssen.html#high-level-composition",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "High-level composition",
    "text": "High-level composition\nTo assess the high-level composition of the reads, I ran the ribodepleted files through Kraken2 and summarized the results with Bracken.\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=library, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Plasma pool\") +\n  theme_kit + \n  theme(plot.title = element_text(hjust=0, face=\"plain\", size=rel(1.5)))\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\ngeom_comp &lt;- purrr::partial(geom_col, position = \"stack\", width = 1)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_comp(data = comp) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, all groups)\")\ng_comp\n\n\n\n\n\n\nCode# Repeat for classified reads only\npalette_assigned &lt;- brewer.pal(9, \"Set1\")[5:9]\ng_comp_assigned &lt;- g_comp_base + \n  geom_comp(data = comp_assigned) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_manual(values=palette_assigned, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, all groups)\")\ng_comp_assigned\n\n\n\n\n\n\nCode# Plot composition of minor components\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_comp(data=comp_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, minor groups)\")\ng_comp_minor\n\n\n\n\n\n\nCodeg_comp_assigned_minor &lt;- g_comp_base + \n  geom_comp(data=comp_assigned_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, minor groups)\")\ng_comp_assigned_minor"
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#total-viral-content",
    "href": "notebooks/2024-07-22-thijssen.html#total-viral-content",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "Total viral content",
    "text": "Total viral content\nTotal viral fraction average \\(2.36 \\times 10^{-4}\\) across samples. As a fraction of assigned (rather than total) reads, this jumped to \\(3.19 \\times 10^{-3}\\):\n\nCodep_reads_viral_all &lt;- comp %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"All reads\")\np_reads_viral_assigned &lt;- comp_assigned %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"Classified reads\")\np_reads_viral &lt;- bind_rows(p_reads_viral_all, p_reads_viral_assigned)\n\n# Plot\ng_viral &lt;- ggplot(p_reads_viral, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_x_discrete(name=\"Plasma pool\") +\n  scale_y_log10(name=\"Viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  guides(color=guide_legend(nrow=2), shape=guide_legend(nrow=2),\n         linetype=guide_legend(nrow=2)) +\n  theme_kit\ng_viral"
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#taxonomic-composition-of-viruses",
    "href": "notebooks/2024-07-22-thijssen.html#taxonomic-composition-of-viruses",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "Taxonomic composition of viruses",
    "text": "Taxonomic composition of viruses\nThe two dominant viruses we see are Anellovirdae and Phycodnaviridae. The threshold for the label “other” are the set of families that make up less than 5% composition in all samples.\n\nCodemajor_threshold &lt;- 0.05\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries, by='sample')\n\n# Plot\ng_families &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_brewer(palette = 'Accent')\ng_families"
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#overall-relative-abundance",
    "href": "notebooks/2024-07-22-thijssen.html#overall-relative-abundance",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "Overall relative abundance",
    "text": "Overall relative abundance\nI calculated the relative abundance of human-infecting viruses in two ways:\n\nFirst, as the total number of deduplicated human-virus reads in each sample, divided by the number of raw reads (“All reads”).\nSecond, as a fraction of preprocessed (cleaned, deduplicated, computationally ribodepleted) reads (“Preprocessed reads”).\n\n\nCode# Get raw read counts\nread_counts_raw &lt;- filter(basic_stats_raw) %&gt;%\n  select(sample, n_reads_raw = n_read_pairs)\nread_counts_preproc &lt;- basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;%\n  select(sample, n_reads_preproc = n_read_pairs)\n\n# Get HV read counts\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% \n  group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=c(\"sample\")) %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0)) %&gt;%\n  left_join(read_counts_preproc, by=c(\"sample\")) %&gt;%\n  inner_join(libraries, by=c(\"sample\")) %&gt;%\n  select(sample, n_reads_raw, n_reads_preproc, n_reads_hv) %&gt;%\n  mutate(n_samples = 1,\n         p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc)\nread_counts_long &lt;- read_counts %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\"))\n\n# Combine for display\nread_counts_agg &lt;- read_counts %&gt;%\n  mutate(p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc) %&gt;%\n  inner_join(libraries, by=c(\"sample\"))\nread_counts_agg_long &lt;- read_counts_agg %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\")) \n\n# Visualize\ng_read_counts &lt;- ggplot(read_counts_agg_long, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_y_log10(name = \"Unique human-viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  theme_kit\ng_read_counts"
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#overall-taxonomy-and-composition",
    "href": "notebooks/2024-07-22-thijssen.html#overall-taxonomy-and-composition",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "Overall taxonomy and composition",
    "text": "Overall taxonomy and composition\nComposition of HV reads was changed from when looking at all viral reads. The two dominant viruses we see are Anellovirdae and Microviridae (bacteriophage). The threshold for the label “other” are the set of families that make up less than 5% composition in all samples.\n\nCodethreshold_major_family &lt;- 0.05\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display) %&gt;%\n  inner_join(libraries, by = 'sample')\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_brewer(palette = 'Accent', name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;%\n  group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv),\n            p_reads_max = max(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\nhv_family_collate"
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html",
    "href": "notebooks/2024-07-23-mengyi.html",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "",
    "text": "This is another potential study of this series. In this post, I analyze Mengyi 2023, a dataset with 200 pooled samples from ~10k samples of blood."
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#high-level-metrics",
    "href": "notebooks/2024-07-23-mengyi.html#high-level-metrics",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "High-level metrics",
    "text": "High-level metrics\nThe average fraction of reads at each stage in the preprocessing pipeline is shown in the following table. Reads lost during trimming and filtering approximately matches what we’d expect based on the raw adapter content. Deduplication loses us about 10% of reads which matched the amount of estimated duplicated reads by QC. Very little ribodepletion was observed which makes sense because this is DNA(?).\n\nCode# TODO: Group by pool size as well\n# Count read losses\nn_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% \n  arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\nThese plots below show the trends from above in each sample.\nTrimming and cleaning gets rid of Illumnia unviersal adapter as well as polyg. Q score remain the same during read cleaning when looking at the positions, with the end of the read actually improving in score. Q scores across all sequences look pretty much the same throughout cleaning.\n\nCodeg_qual &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,5),\n                     expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~., scales = \"free_y\")\ng_quality_seq\n\n\n\n\n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, group=sample)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, \n                      aes(x=stage, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nAll samples tend to follow similar trends for deduplication, with a large decrease in read length post adapter trimming.\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages\n\n\n\n\n\n\n\nRibosomal reads were quite low, near 0% for every sample.\n\nCode# Calculate reads lost during ribodepletion (approximation for % ribosomal reads)\nreads_ribo &lt;- n_reads_rel %&gt;% \n  filter(stage %in% c(\"dedup\", \"ribo_secondary\")) %&gt;% \n  group_by(sample) %&gt;% \n  summarize(p_reads_ribo=1-n_read_pairs[2]/n_read_pairs[1], .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\nreads_ribo_summ &lt;- reads_ribo %&gt;%\n  group_by(sample) %&gt;%\n  summarize(min=min(p_reads_ribo), max=max(p_reads_ribo),\n            mean=mean(p_reads_ribo), .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\ng_reads_ribo &lt;- ggplot(reads_ribo, \n                       aes(x=library, y=p_reads_ribo)) +\n  geom_point() + \n  scale_y_continuous(name=\"Approx % ribosomal reads\", limits=c(0,1),\n                     breaks=seq(0,1,0.2), expand=c(0,0), labels = function(y) y*100)+\n  theme_kit\ng_reads_ribo"
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#high-level-composition",
    "href": "notebooks/2024-07-23-mengyi.html#high-level-composition",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "High-level composition",
    "text": "High-level composition\nTo assess the high-level composition of the reads, I ran the ribodepleted files through Kraken2 and summarized the results with Bracken.\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=library, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Plasma pool\") +\n  theme_kit + \n  theme(plot.title = element_text(hjust=0, face=\"plain\", size=rel(1.5)))\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\ngeom_comp &lt;- purrr::partial(geom_col, position = \"stack\", width = 1)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_comp(data = comp) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, all groups)\")\ng_comp\n\n\n\n\n\n\nCode# Repeat for classified reads only\npalette_assigned &lt;- brewer.pal(9, \"Set1\")[5:9]\ng_comp_assigned &lt;- g_comp_base + \n  geom_comp(data = comp_assigned) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_manual(values=palette_assigned, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, all groups)\")\ng_comp_assigned\n\n\n\n\n\n\nCode# Plot composition of minor components\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_comp(data=comp_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, minor groups)\")\ng_comp_minor\n\n\n\n\n\n\nCodeg_comp_assigned_minor &lt;- g_comp_base + \n  geom_comp(data=comp_assigned_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, minor groups)\")\ng_comp_assigned_minor"
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#total-viral-content",
    "href": "notebooks/2024-07-23-mengyi.html#total-viral-content",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "Total viral content",
    "text": "Total viral content\nTotal viral fraction average \\(1.03 \\times 10^{-4}\\) across samples. As a fraction of assigned (rather than total) reads, this jumped to \\(1.28 \\times 10^{-4}\\):\n\nCodep_reads_viral_all &lt;- comp %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"All reads\")\np_reads_viral_assigned &lt;- comp_assigned %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"Classified reads\")\np_reads_viral &lt;- bind_rows(p_reads_viral_all, p_reads_viral_assigned)\n\n# Plot\ng_viral &lt;- ggplot(p_reads_viral, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_x_discrete(name=\"Plasma pool\") +\n  scale_y_log10(name=\"Viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  guides(color=guide_legend(nrow=2), shape=guide_legend(nrow=2),\n         linetype=guide_legend(nrow=2)) +\n  theme_kit\ng_viral\n\nWarning: Transformation introduced infinite values in continuous y-axis"
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#taxonomic-composition-of-viruses",
    "href": "notebooks/2024-07-23-mengyi.html#taxonomic-composition-of-viruses",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "Taxonomic composition of viruses",
    "text": "Taxonomic composition of viruses\nThe one dominant viruses we see is Hepadnaviridae. The threshold for the label “other” are the set of families that make up less than 20% composition in all samples (the only reason I did this was because there were too many matches).\n\nCodemajor_threshold &lt;- 0.20\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries, by='sample')\n\n# Create a custom color palette with up to 20 colors\ncustom_palette &lt;- c(\n  brewer.pal(8, \"Set2\"),\n  brewer.pal(8, \"Set1\"),\n  brewer.pal(4, \"Pastel1\")\n)\n\n# Plot\ng_families &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values = custom_palette)\ng_families"
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#overall-relative-abundance",
    "href": "notebooks/2024-07-23-mengyi.html#overall-relative-abundance",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "Overall relative abundance",
    "text": "Overall relative abundance\nI calculated the relative abundance of human-infecting viruses in two ways:\n\nFirst, as the total number of deduplicated human-virus reads in each sample, divided by the number of raw reads (“All reads”).\nSecond, as a fraction of preprocessed (cleaned, deduplicated, computationally ribodepleted) reads (“Preprocessed reads”).\n\n\nCode# Get raw read counts\nread_counts_raw &lt;- filter(basic_stats_raw) %&gt;%\n  select(sample, n_reads_raw = n_read_pairs)\nread_counts_preproc &lt;- basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;%\n  select(sample, n_reads_preproc = n_read_pairs)\n\n# Get HV read counts\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% \n  group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=c(\"sample\")) %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0)) %&gt;%\n  left_join(read_counts_preproc, by=c(\"sample\")) %&gt;%\n  inner_join(libraries, by=c(\"sample\")) %&gt;%\n  select(sample, n_reads_raw, n_reads_preproc, n_reads_hv) %&gt;%\n  mutate(n_samples = 1,\n         p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc)\nread_counts_long &lt;- read_counts %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\"))\n\n# Combine for display\nread_counts_agg &lt;- read_counts %&gt;%\n  mutate(p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc) %&gt;%\n  inner_join(libraries, by=c(\"sample\"))\nread_counts_agg_long &lt;- read_counts_agg %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\")) \n\n# Visualize\ng_read_counts &lt;- ggplot(read_counts_agg_long, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_y_log10(name = \"Unique human-viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  theme_kit\ng_read_counts"
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#overall-taxonomy-and-composition",
    "href": "notebooks/2024-07-23-mengyi.html#overall-taxonomy-and-composition",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "Overall taxonomy and composition",
    "text": "Overall taxonomy and composition\nComposition of HV reads was changed from when looking at all viral reads. The two dominant viruses we see are Anellovirdae and Hepadnaviridae. The threshold for the label “other” are the set of families that make up less than 5% composition in all samples.\n\nCodethreshold_major_family &lt;- 0.05\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display) %&gt;%\n  inner_join(libraries, by = 'sample')\n\n# Create a custom color palette with up to 20 colors\ncustom_palette &lt;- c(\n  brewer.pal(8, \"Set2\"),\n  brewer.pal(8, \"Set1\"),\n  brewer.pal(4, \"Pastel1\")\n)\n\n# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values = custom_palette, name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;%\n  group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv),\n            p_reads_max = max(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\nhv_family_collate"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#number-of-reads-for-viruses-of-interest",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#number-of-reads-for-viruses-of-interest",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Number of reads for viruses of interest",
    "text": "Number of reads for viruses of interest\nWe’re particulary interested in the following viruses:\n\nHepatitis B\n\nFamily: Hepadnaviridae\nGenus: Orthohepadnavirus\nSpecies: Hepatitis B virus\n\n\nHepatitis C\n\nFamily: Flaviviridae\nGenus: Hepacivirus\nSpecies: Hepatitis C virus\n\n\nEBV (Epstein-Barr virus)\n\nFamily: Herpesviridae\nGenus: Lymphocryptovirus\nSpecies: Human gammaherpesvirus 4\n\n\nCMV (Cytomegalovirus)\n\nFamily: Herpesviridae\nGenus: Cytomegalovirus\nSpecies: Human betaherpesvirus 5\n\n\nHIV-1 (Human Immunodeficiency Virus type 1)\n\nFamily: Retroviridae\nGenus: Lentivirus\nSpecies: Human immunodeficiency virus 1\n\n\nAnelloviridae\n\nFamily: Anelloviridae\n\n\n\n\nCode#reads_post_process &lt;- basic_stats %&gt;% filter(stage == 'ribo_secondary') %&gt;% \n  #select(sample, n_read_pairs)\n\nviral_families_interest &lt;- viral_families %&gt;% filter(name == \"Hepadnaviridae\" | \n    name ==  \"Flaviviridae\" | \n    name ==  \"Orthoherpesviridae\" |\n    name ==  \"Herpesviridae\" |\n    name ==  \"Retroviridae\" |\n    name == \"Anelloviridae\") %&gt;% group_by(name) %&gt;% summarize(sum=sum(n_reads_clade))\n\nviral_families_interest\n\n\n  \n\n\n\n\nCode# Species\n# Hepatitis B virus &lt;- 10407\n# Hepatitis C virus &lt;- 10407\n# HIV - 1 &lt;- 11676\n\n# viral_taxa %&gt;% filter(str_detect(tolower(name),'hepatitis') | str_detect(tolower(name),'human immunodeficiency virus') | str_detect(tolower(name),'gammaherpesvirus') | str_detect(tolower(name),'betaherpesvirus'))\n\nviral_genus_interest &lt;- kraken_reports_viral_cleaned %&gt;% filter(str_detect(tolower(name),'orthohepadnavirus') | str_detect(tolower(name),'hepacivirus') | str_detect(tolower(name),'lymphocryptovirus') | str_detect(tolower(name),'cytomegalovirus') | str_detect(tolower(name), 'lentivirus')) %&gt;% group_by(name) %&gt;% summarize(sum=sum(n_reads_clade))\n\nviral_genus_interest\n\n\n  \n\n\n\n\nCode# Species\n# Hepatitis B virus &lt;- 10407\n# Hepatitis C virus &lt;- 10407\n# HIV - 1 &lt;- 11676\n\n# viral_taxa %&gt;% filter(str_detect(tolower(name),'hepatitis') | str_detect(tolower(name),'human immunodeficiency virus') | str_detect(tolower(name),'gammaherpesvirus') | str_detect(tolower(name),'betaherpesvirus'))\n\nviral_species_interest &lt;- kraken_reports_viral_cleaned %&gt;% filter(str_detect(tolower(name),'hepatitis') | str_detect(tolower(name),'human immunodeficiency virus') | str_detect(tolower(name),'gammaherpesvirus') | str_detect(tolower(name),'betaherpesvirus')) %&gt;% group_by(name) %&gt;% summarize(sum=sum(n_reads_clade))\n\nviral_species_interest"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#about",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#about",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "About",
    "text": "About\nThis dataset is composed of 60 samples which come from plasma pools of anywhere from 8-13 people from Spain. In total, there were 567 healthy individuals that have contributed to these pools. The demographic information that I have for each pool is the gender and age of the donor as well as the number that has contributed to each pool. For each pooled sample, they performed a combined DNA and RNA library preparation, resulting in a single sequencing output that captures both nucleic acid types. This approach provides comprehensive genetic information but precludes separate analysis of DNA and RNA from individual samples.\nSample + library preparation\nA total of 587 plasma samples from healthy donors were collected from the Centro de Transfusión de la Comunidad Valenciana (Valencia, Spain) from 15 September 2018 to 30 March 2019 and stored at −80 °C until use.\nEach of the 60 pools (SP1-SP60) analyzed in this study was obtained by mixing 1 mL of plasma from a variable number of donors (between 8- and 13-mL total). To assess viral recovery, each pool was spiked with 103 PFU of ϕX174 and 104 PFU of vesicular stomatitis virus (VSV). The purification protocol has been previously described in detail [18]. Briefly, plasma pools were processed with 1.0 µM filters to remove cells and other non-viral particles and the filtered fractions were subject to high-speed centrifugation (87,000 g, 2 h, 4 °C), washed with PBS 1X (87,000 g, 1 h, 4 °C), and resuspended in 245 µL 1X digestion buffer (Turbo DNA Free kit, Ambion, Carlsbad, CA, USA). Then, 5 µL of Turbo DNase, 2 µL of Benzonase (Sigma, Darmstadt, Germany) and 2 µL of micrococcal nuclease (NEB) were added to the sample to remove unprotected nucleic acids. After incubation (1 h, 37 °C), 20 µL of stop reagent was added, following the manufacturer’s instructions. Then, 240 µL supernatant was transferred to a new tube and split into two fractions: 200 µL fraction was used for RNA extraction using TRIzol LS reagent (Invitrogen, Carlsbad, USA), followed by purification with the QIAamp Viral RNA Mini kit (Qiagen, Hilden, Germany) and amplification with the QuantiTect Whole Transcriptome kit (Qiagen), and 40 µL fraction was used for DNA extraction with the QIAamp Viral RNA Mini kit and amplification with the TruePrime WGA kit (Sygnis, Heidelberg, Germany).\nFor each pool, DNA and RNA amplification products were mixed in equimolar concentration before library preparation, which was carried out using Nextera XT DNA library preparation kit with 15 amplification cycles (Illumina, San Diego, USA), and subject to pair-end sequencing in a NextSeq device.\nMore details can be found in their original paper which outlined this protocol."
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#quality-control",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#quality-control",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Quality control",
    "text": "Quality control\nIn total, these 60 samples contained 230M read pairs. The samples had 2.3M - 4.8M (mean 3.8M) read pairs each.\nThe number of read pairs and total base pairs looks good, however the duplication rate is quite high (this can be attributed to their sample preparation).\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(library,\n         pool_size,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(library:pool_size), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric)) \n\n# Set up plot templates\n\ng_basic &lt;- ggplot(basic_stats_raw_metrics, aes(x = library, y = value, fill = pool_size)) +\n  geom_col(position = \"dodge\") +\n  scale_x_discrete() +\n  scale_y_continuous(expand = c(0, 0)) +\n  expand_limits(y = c(0, 100)) +\n  scale_fill_brewer(palette = \"Accent\") + \n  facet_grid(metric ~ ., scales = \"free\", space = \"free_x\", switch = \"y\") +\n  theme_kit + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face = \"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\nAdapter content is low (the upper limit on the plot is 1%). Read quality seems good both over all the positions as well as over the number of sequences.\n\nCode# Set up plotting templates\ng_qual_raw &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,1),\n                     expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#species-analysis",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#species-analysis",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Species analysis",
    "text": "Species analysis\nTo get a good overview of families, genera, and species, we can look at a Sankey plot where the magnitude of relative abundance, averaged over all samples, is shown in parentheses.\n\nCode# Function to create links\ncreate_links &lt;- function(data) {\n  family_to_genus &lt;- data %&gt;%\n    filter(!is.na(genus)) %&gt;%\n    group_by(family, genus) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = family, target = genus)\n  \n  genus_to_species &lt;- data %&gt;%\n    group_by(genus, species) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = genus, target = species)\n\n  family_to_species &lt;- data %&gt;%\n    filter(is.na(genus)) %&gt;%\n    group_by(family, species) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = family, target = species)\n\n  bind_rows(family_to_genus, genus_to_species, family_to_species) %&gt;%\n    filter(!is.na(source))\n}\n\n# Function to create nodes\ncreate_nodes &lt;- function(links) {\n  data.frame(\n    name = c(links$source, links$target) %&gt;% unique()\n  )\n}\n\n# Function to prepare data for Sankey diagram\nprepare_sankey_data &lt;- function(links, nodes) {\n  links$IDsource &lt;- match(links$source, nodes$name) - 1\n  links$IDtarget &lt;- match(links$target, nodes$name) - 1\n  list(links = links, nodes = nodes)\n}\n\n# Function to create Sankey plot\ncreate_sankey_plot &lt;- function(sankey_data) {\n  sankeyNetwork(\n    Links = sankey_data$links, \n    Nodes = sankey_data$nodes,\n    Source = \"IDsource\", \n    Target = \"IDtarget\",\n    Value = \"value\", \n    NodeID = \"name\",\n    sinksRight = TRUE,\n    nodeWidth = 50,\n    fontSize = 14,\n    height = 800,\n    width = 1000\n  )\n}\n\nsave_sankey_as_png &lt;- function(sankey_plot, width = 1000, height = 800) {\n  # Save the plot as an HTML file\n  saveWidget(sankey_plot, sprintf('%s/sankey.html',image_dir))\n}\n\nformat_scientific &lt;- function(x, digits=2) {\n  sapply(x, function(val) {\n    if (is.na(val) || abs(val) &lt; 1e-15) {\n      return(\"0\")\n    } else {\n      exponent &lt;- floor(log10(abs(val)))\n      coef &lt;- val / 10^exponent\n      return(sprintf(\"%.1f × 10^%d\", round(coef, digits), exponent))\n    }\n  })\n}\n\ndata &lt;- result %&gt;% \n  mutate(across(c(genus_n_reads_tot, genus_ra_reads_tot), ~replace_na(., 0)),\n         genus = ifelse(is.na(genus), \"Unknown Genus\", genus)) %&gt;%\n  mutate(\n  species = paste0(species, sprintf(' (%s)', format_scientific(species_ra_reads_tot))),\n  genus = paste0(genus, sprintf(' (%s)', format_scientific(genus_ra_reads_tot))),\n  family = paste0(family, sprintf(' (%s)', format_scientific(family_ra_reads_tot)))\n)\nlinks &lt;- as.data.frame(create_links(data))\nnodes &lt;- create_nodes(links)\nsankey_data &lt;- prepare_sankey_data(links, nodes)\nsankey &lt;- create_sankey_plot(sankey_data)\n\nsankey\n\n\n\n\nCode#save_sankey_as_png(sankey)\n\n\nTo get a better idea of the relative abundance of species, we can create a dot plot where each dot represents the relative abundance of a particular species in a sample.\n\nCodespecies_family &lt;- result %&gt;% select(species, family) %&gt;% rename('name' = 'species')\n\nplay &lt;- hv_species_counts %&gt;% \n  ungroup() %&gt;%\n  inner_join(libraries, by = 'sample') %&gt;%\n  inner_join(species_family, by = 'name')\n\nplay$family &lt;- factor(play$family, levels=hv_family_collate %&gt;% pull(family))\nplay &lt;- play %&gt;%\n  arrange(family, name) %&gt;%\n  mutate(name = factor(name, levels = unique(name)))\n#name_order &lt;- play %&gt;% arrange(family) %&gt;% pull(name)\n#play$name &lt;- factor(play$name, levels = name_order)\n\nra_dot &lt;- ggplot(play, aes(x = ra_reads_hv, y=name)) +\n  geom_quasirandom(orientation = 'y', aes(color = family)) +\n   scale_color_brewer(palette = \"Accent\") +\n    scale_x_log10(\n    \"Relative abundance human virus reads\",\n    labels = function(x) parse(text = paste0(\"10^\", round(log10(x)))),\n    limits = c(1e-7, 1),\n    n.breaks = 8\n  ) +\n  labs(y = 'Human virus',\n       color = 'Viral family') + \n  theme_light() + \n    theme(\n    axis.text.y = element_text(size = 8),\n    axis.text.x = element_text(size = 14),\n    axis.ticks.y = element_blank(),axis.line = element_line(colour = \"black\"),\n    axis.title.x = element_text(size = 15),    \n    axis.title.y = element_text(size = 15),  \n    legend.text = element_text(size = 13),\n    legend.title = element_text(size = 16),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank())\nra_dot\n\n\n\n\n\n\nCode#ggsave(sprintf('%s/ra_viral_dot.jpg',image_dir), ra_dot, height = 8 ,width = 12)\n\n\nI’d like to BLAST some of these reads against the NCBI nt database to see if we can get some more info on them."
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#about",
    "href": "notebooks/2024-07-22-thijssen.html#about",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "About",
    "text": "About\nThis dataset is composed of 20 samples which come from plasma pools of 5 people from Iran. In total, there were 100 healthy individuals that contributed to this pool. For each pooled sample, they performed a combined DNA and RNA library preparation, resulting in a single sequencing output that captures both nucleic acid types. This approach provides comprehensive genetic information but precludes separate analysis of DNA and RNA from individual samples.\nSample + library preparation\nThis cross-sectional study was conducted from 2017 to 2018 in the Boushehr Province, Iran. The individuals were recruited in one of the five transfusion clinics located in Boushehr. In total 100 healthy blood donors were included. Seven milliliters of blood were collected from each individual during blood transfusion or donation. Immediately after collection, plasma was separated from the samples and stored at −70 °C. Plasma samples were shipped under freezing conditions to the Laboratory for Clinical and Epidemiological Virology, Rega Institute, KU Leuven, Belgium.\nUpon arrival, the samples were processed in the laboratory. Initially, the samples were centrifuged and 100 µL of the supernatant was pooled with five samples per pool. The pooled plasma samples were subjected to an adapted version of the NetoVIR protocol for viral particle enrichment and metagenomic sequencing 20. To control for laboratory and environment contamination, negative controls were included in the different steps of the sample preparation procedure and pooled for sequencing. Pooled plasma samples and negative controls (H2O) were centrifuged for 3 min at 17,000× g and filtered through 0.8 µm polyether sulphone filters (Sartorius, Göttingen, Germany). To remove free-floating nucleic acids, the filtered samples were subjected to a nuclease treatment with a cocktail of 1 µL micrococcal nuclease (New England Biolabs, Ipswich, MA, USA) and 2 µL benzonase (Millipore, Burlington, United States) for 2 h at 37 °C. Both viral DNA and RNA were extracted (Viral RNA minikit, Qiagen, Hilden, Germany) and randomly amplified (including primary step of reverse transcription) with the Whole Transcriptome Amplification 2 kit (WTA2, Sigma Aldrich, Darmstadt, Germany) for 20 cycles.\nThe amplification product was purified with the MSB SPIN PCRAPACE kit (Stratec, Birkenfeld, Germany) and prepared for sequencing by using the Nextera XT kit (Illumina, San Diego, CA, USA). DNA products were quantified with the Qubit fluorometer (Thermo Fisher Scientific, Waltham, MA, USA), and the High-Sensitivity DNA kit (Agilent, Ratingen, Germany) for the Bioanalyzer 2100 (Agilent, Ratingen, Germany) was used to determine the average library fragment size. Samples were pooled in equimolar ratios, and paired-end sequencing (2 × 150 bp) was performed on a NextSeq 500 Illumina platform (Nucleomics Core, Leuven, Belgium) with an average of 10 million reads per sample."
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#quality-control",
    "href": "notebooks/2024-07-22-thijssen.html#quality-control",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "Quality control",
    "text": "Quality control\nIn total, these 20 samples contained ~116M read pairs. The samples had 1.3M - 11.1M (mean ~5.8M) read pairs each.\nThe number of reads looks pretty good, although a few samples have a much lower number of reads. Considering the authors used these samples, we’ll use the same. The total number of base pairs also looks reasonable, matching the trends of the number of reads. The duplication rate is low.\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(library,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-library, names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric)) \n\n# Set up plot templates\n\ng_basic &lt;- ggplot(basic_stats_raw_metrics, aes(x = library, y = value)) +\n  geom_col(position = \"dodge\") +\n  scale_x_discrete() +\n  scale_y_continuous(expand = c(0, 0)) +\n  expand_limits(y = c(0, 100)) +\n  facet_grid(metric ~ ., scales = \"free\", space = \"free_x\", switch = \"y\") +\n  theme_kit + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face = \"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\nAdapter content is quite high for nextera-transposase-sequence. I believe this is library contamination, I need to look into this TODO. As we’d expect, we see higher quality Q scores near the beginning of the read and a gradual decline towards the end of the read, however all positions seem to have a Q score of 35 which means that our reads are ~99.97% accurate. When looking at the Q score over all sequences, we can see a sharp peak around 35, which corresponds to the previous plot, indicating high read quality.\n\nCode# Set up plotting templates\ng_qual_raw &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,NA),\n                     breaks = seq(0,100,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw"
  },
  {
    "objectID": "notebooks/2024-07-22-thijssen.html#species-analysis",
    "href": "notebooks/2024-07-22-thijssen.html#species-analysis",
    "title": "Workflow of Thijssen et al. (2023)",
    "section": "Species analysis",
    "text": "Species analysis\nTo get a good overview of families, genera, and species, we can look at a Sankey plot where the magnitude of relative abundance, averaged over all samples, is shown in parentheses.\n\nCode# Function to create links\ncreate_links &lt;- function(data) {\n  family_to_genus &lt;- data %&gt;%\n    filter(!is.na(genus)) %&gt;%\n    group_by(family, genus) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = family, target = genus)\n  \n  genus_to_species &lt;- data %&gt;%\n    group_by(genus, species) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = genus, target = species)\n\n  family_to_species &lt;- data %&gt;%\n    filter(is.na(genus)) %&gt;%\n    group_by(family, species) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = family, target = species)\n\n  bind_rows(family_to_genus, genus_to_species, family_to_species) %&gt;%\n    filter(!is.na(source))\n}\n\n# Function to create nodes\ncreate_nodes &lt;- function(links) {\n  data.frame(\n    name = c(links$source, links$target) %&gt;% unique()\n  )\n}\n\n# Function to prepare data for Sankey diagram\nprepare_sankey_data &lt;- function(links, nodes) {\n  links$IDsource &lt;- match(links$source, nodes$name) - 1\n  links$IDtarget &lt;- match(links$target, nodes$name) - 1\n  list(links = links, nodes = nodes)\n}\n\n# Function to create Sankey plot\ncreate_sankey_plot &lt;- function(sankey_data) {\n  sankeyNetwork(\n    Links = sankey_data$links, \n    Nodes = sankey_data$nodes,\n    Source = \"IDsource\", \n    Target = \"IDtarget\",\n    Value = \"value\", \n    NodeID = \"name\",\n    sinksRight = TRUE,\n    nodeWidth = 25,\n    fontSize = 14,\n  )\n}\n\nsave_sankey_as_png &lt;- function(sankey_plot, width = 1000, height = 800) {\n  # Save the plot as an HTML file\n  saveWidget(sankey_plot, sprintf('%s/sankey.html',data_dir))\n}\n\nformat_scientific &lt;- function(x, digits=2) {\n  sapply(x, function(val) {\n    if (is.na(val) || abs(val) &lt; 1e-15) {\n      return(\"0\")\n    } else {\n      exponent &lt;- floor(log10(abs(val)))\n      coef &lt;- val / 10^exponent\n      #return(sprintf(\"%.1f × 10^%d\", round(coef, digits), exponent))\n      # May or may not be smart, just keeping magnitude\n      return(sprintf(\"10^%d\", exponent))\n    }\n  })\n}\n\ndata &lt;- result %&gt;% \n  mutate(across(c(genus_n_reads_tot, genus_ra_reads_tot), ~replace_na(., 0)),\n         genus = ifelse(is.na(genus), \"Unknown Genus\", genus)) %&gt;%\n  mutate(\n  species = paste0(species, sprintf(' (%s)', format_scientific(species_ra_reads_tot))),\n  genus = paste0(genus, sprintf(' (%s)', format_scientific(genus_ra_reads_tot))),\n  family = paste0(family, sprintf(' (%s)', format_scientific(family_ra_reads_tot)))\n)\nlinks &lt;- as.data.frame(create_links(data))\nnodes &lt;- create_nodes(links)\nsankey_data &lt;- prepare_sankey_data(links, nodes)\nsankey &lt;- create_sankey_plot(sankey_data)\n\nsankey\n\n\n\n\n\nTo get a better idea of the relative abundance of species, we can create a dot plot where each dot represents the relative abundance of a particular species in a sample.\n\nCodespecies_family &lt;- result %&gt;% select(species, family) %&gt;% rename('name' = 'species')\n\nplay &lt;- hv_species_counts %&gt;% \n  ungroup() %&gt;%\n  inner_join(libraries, by = 'sample') %&gt;%\n  inner_join(species_family, by = 'name')\n\nplay$family &lt;- factor(play$family, levels=hv_family_collate %&gt;% pull(family))\nplay &lt;- play %&gt;%\n  arrange(family, name) %&gt;%\n  mutate(name = factor(name, levels = unique(name)))\n#name_order &lt;- play %&gt;% arrange(family) %&gt;% pull(name)\n#play$name &lt;- factor(play$name, levels = name_order)\n\npal &lt;- c(brewer.pal(8, 'Dark2'),brewer.pal(8, 'Accent'))\n\nra_dot &lt;- ggplot(play, aes(x = ra_reads_hv, y=name)) +\n  geom_quasirandom(orientation = 'y', aes(color = family)) +\n  scale_color_manual(values = pal) + \n    scale_x_log10(\n    \"Relative abundance human virus reads\",\n    labels = function(x) parse(text = paste0(\"10^\", round(log10(x)))),\n    limits = c(1e-7, 1),\n    n.breaks = 8\n  ) +\n  labs(y = 'Human virus',\n       color = 'Viral family') + \n  theme_light() + \n    theme(\n    axis.text.y = element_text(size = 8),\n    axis.text.x = element_text(size = 14),\n    axis.ticks.y = element_blank(),axis.line = element_line(colour = \"black\"),\n    axis.title.x = element_text(size = 15),    \n    axis.title.y = element_text(size = 15),  \n    legend.text = element_text(size = 13),\n    legend.title = element_text(size = 16),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank())\nra_dot\n\n\n\n\n\n\n\nWe now exclude Anelloviridae from the plot.\n\nCodeplay_special &lt;- play %&gt;% filter(family != 'Anelloviridae')\n\npal &lt;- c(brewer.pal(8, 'Dark2'),brewer.pal(8, 'Accent'))\n\nra_dot &lt;- ggplot(play_special, aes(x = ra_reads_hv, y=name)) +\n  geom_quasirandom(orientation = 'y', aes(color = family)) +\n  scale_color_manual(values = pal) + \n    scale_x_log10(\n    \"Relative abundance human virus reads\",\n    labels = function(x) parse(text = paste0(\"10^\", round(log10(x)))),\n    limits = c(1e-7, 1),\n    n.breaks = 8\n  ) +\n  labs(y = 'Human virus',\n       color = 'Viral family') + \n  theme_light() + \n    theme(\n    axis.text.y = element_text(size = 8),\n    axis.text.x = element_text(size = 14),\n    axis.ticks.y = element_blank(),axis.line = element_line(colour = \"black\"),\n    axis.title.x = element_text(size = 15),    \n    axis.title.y = element_text(size = 15),  \n    legend.text = element_text(size = 13),\n    legend.title = element_text(size = 16),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank())\nra_dot\n\n\n\n\n\n\n\nI can then take all of the viruses that we found and look up what they’re responsible for and whether they’re dangerous.\n\n\n\n\n\n\n\nVirus Name\nCommon Name\nPathogenic Potential\n\n\n\nRotavirus A\nRotavirus\nHigh, causes severe diarrhea in children\n\n\nEnterovirus C\nIncludes poliovirus and some coxsackieviruses\nHigh, can cause various diseases including polio\n\n\nSevere acute respiratory syndrome-related coronavirus\nSARS coronavirus\nHigh, causes severe respiratory illness\n\n\nSapporo virus\nSapporo virus or Sapovirus\nModerate, causes gastroenteritis\n\n\nHuman mastadenovirus F\nAdenovirus F\nModerate, can cause gastroenteritis\n\n\nAlphapolyomavirus quintihominis\nNot widely known\nLow, but some polyomaviruses can cause disease in immunocompromised individuals\n\n\nAlphapapillomavirus 4\nHuman papillomavirus (HPV) type 4\nLow to moderate, can cause warts\n\n\nGokushovirus WZ-2015a\nNot widely known\nLow, typically infects bacteria\n\n\nHuman gut gokushovirus\nNot widely known\nLow, typically infects bacteria in human gut\n\n\nMicroviridae sp.\nNot widely known\nLow, typically infects bacteria\n\n\nMicrovirus sp.\nNot widely known\nLow, typically infects bacteria\n\n\nAmbidensovirus sp.\nNot widely known\nLow for humans, can be pathogenic to insects\n\n\nDependoparvovirus primate1\nAdeno-associated virus (AAV)\nLow, not known to cause disease in humans\n\n\nVaccinia virus\nVaccinia virus\nLow to moderate, used as smallpox vaccine\n\n\nCrAssphage ZA\nNot widely known\nLow, typically infects bacteria in human gut\n\n\nInovirus D_HF3_19\nNot widely known\nLow, typically infects bacteria\n\n\nInovirus D_HF5_49\nNot widely known\nLow, typically infects bacteria\n\n\nCardiovirus D\nNot widely known\nUnknown, some cardioviruses can cause disease in animals\n\n\nCosavirus A\nNot widely known\nUnknown, potentially associated with gastroenteritis\n\n\n\nI’ve ordered these from pathogenic potential to not. We do get coronavirus, however when we look up the taxid on NCBI, we get the following which seems to be an id for general corona and not SARS-CoV-2.\nI’d like to BLAST some of these reads against the NCBI nt database to see if we can get some more info on them."
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#about",
    "href": "notebooks/2024-07-23-mengyi.html#about",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "About",
    "text": "About\nThis dataset is composed of 200 samples which come from plasma pools of 160 random blood samples from seven different locations in China between 2012-2018. In total, there were 10,720 blood samples (they do not list number of people). They did DNA sequencing for each sample.\nSample + library preparation\nFrom January 1, 2012, to December 31, 2018, a total of 10,720 blood samples of 10 ml each were randomly selected from voluntary blood donors in 7 regions (the distribution of sampling location and a corresponding number of samples are shown in the Fig. 1). The blood samples taken from various places were mixed in units of 160 (each 100 μl) for ultracentrifugation (32,000 rpm, 120 min, maximum centrifugal radius of 91.9 mm). Afterward, we rinsed and resuspended the precipitate with 500 μl PBS.\nThe pooled suspensions were subjected to extraction of total DNA using QIAamp® DNA Blood mini Kit (QIAGEN Cat. NO.160019269, Frankfurt, Germany), DNA concentration was measured by Equalbit® 1 × dsDNA HS Assay Kit (Vazyme Cat. NO. 7E302K9, Nanjing, China).\nThe metagenomic library was constructed using KAPA HyperPlus Kit (KAPA Cat. NO. 0000097583, Boston, USA) with dual-indexed Adapters (KAPA Cat. NO. 0000093370, Boston, USA), the DNA was fragmented to 250 bp approximately by the enzyme at 37 °C for 20 min, after end repair and A-tailing, adapter ligation, post-ligation cleanup, library amplification, and post-amplification cleanup, the library was constructed.\nAgilent 2100 Bioanalyzer (Agilent Technologies, Beijing, China) was used for library quality control, and qualified DNA library was sent to the Novogene company to sequence in HiSeq 4500."
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#quality-control",
    "href": "notebooks/2024-07-23-mengyi.html#quality-control",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "Quality control",
    "text": "Quality control\nIn total, these 200 samples contained 3.4B read pairs. The samples had 8.2M - 30.1M (mean 17.1M) read pairs each.\nNumber of read pairs, total bases and % of duplicates all look good.\n\nCode# Prepare data\nbasic_stats_raw_metrics &lt;- basic_stats_raw %&gt;%\n  select(library,\n         location,\n         `# Read pairs` = n_read_pairs,\n         `Total base pairs\\n(approx)` = n_bases_approx,\n         `% Duplicates\\n(FASTQC)` = percent_duplicates) %&gt;%\n  pivot_longer(-(library:location), names_to = \"metric\", values_to = \"value\") %&gt;%\n  mutate(metric = fct_inorder(metric)) \n\n# Set up plot templates\n\ng_basic &lt;- ggplot(basic_stats_raw_metrics, aes(x = library, y = value, fill=location)) +\n  geom_col(position = \"dodge\") +\n  scale_x_discrete() +\n  scale_fill_brewer(palette = \"Accent\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  expand_limits(y = c(0, 100)) +\n  facet_grid(metric ~ ., scales = \"free\", space = \"free_x\", switch = \"y\") +\n  theme_kit + theme(\n    axis.title.y = element_blank(),\n    strip.text.y = element_text(face = \"plain\")\n  )\ng_basic\n\n\n\n\n\n\n\nAdapter content is low. As we’d expect, we see higher quality Q scores near the beginning of the read and a gradual decline towards the end of the read, however all positions seem to have a Q score of 35 which means that our reads are ~99.97% accurate. When looking at the Q score over all sequences, we can see a sharp peak after 35, which closely follows the previous plot, indicating high read quality.\n\nCode# Set up plotting templates\ng_qual_raw &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters_raw &lt;- g_qual_raw + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats_raw) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,5),\n                     expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0)) +\n  facet_grid(.~adapter)\ng_adapters_raw\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base_raw &lt;- g_qual_raw +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,500,20), expand=c(0,0))\ng_quality_base_raw\n\n\n\n\n\n\nCodeg_quality_seq_raw &lt;- g_qual_raw +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0))\ng_quality_seq_raw"
  },
  {
    "objectID": "notebooks/2024-07-23-mengyi.html#species-analysis",
    "href": "notebooks/2024-07-23-mengyi.html#species-analysis",
    "title": "Workflow of Mengyi et al. (2023)",
    "section": "Species analysis",
    "text": "Species analysis\nTo get a good overview of families, genera, and species, we can look at a Sankey plot where the magnitude of relative abundance, averaged over all samples, is shown in parentheses.\n\nCode# Function to create links\ncreate_links &lt;- function(data) {\n  family_to_genus &lt;- data %&gt;%\n    filter(!is.na(genus)) %&gt;%\n    group_by(family, genus) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = family, target = genus)\n  \n  genus_to_species &lt;- data %&gt;%\n    group_by(genus, species) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = genus, target = species)\n\n  family_to_species &lt;- data %&gt;%\n    filter(is.na(genus)) %&gt;%\n    group_by(family, species) %&gt;%\n    summarise(value = n(), .groups = \"drop\") %&gt;%\n    mutate(source = family, target = species)\n\n  bind_rows(family_to_genus, genus_to_species, family_to_species) %&gt;%\n    filter(!is.na(source))\n}\n\n# Function to create nodes\ncreate_nodes &lt;- function(links) {\n  data.frame(\n    name = c(links$source, links$target) %&gt;% unique()\n  )\n}\n\n# Function to prepare data for Sankey diagram\nprepare_sankey_data &lt;- function(links, nodes) {\n  links$IDsource &lt;- match(links$source, nodes$name) - 1\n  links$IDtarget &lt;- match(links$target, nodes$name) - 1\n  list(links = links, nodes = nodes)\n}\n\n# Function to create Sankey plot\ncreate_sankey_plot &lt;- function(sankey_data) {\n  sankeyNetwork(\n    Links = sankey_data$links, \n    Nodes = sankey_data$nodes,\n    Source = \"IDsource\", \n    Target = \"IDtarget\",\n    Value = \"value\", \n    NodeID = \"name\",\n    sinksRight = TRUE,\n    nodeWidth = 25,\n    fontSize = 14,\n  )\n}\n\nsave_sankey_as_png &lt;- function(sankey_plot, width = 1000, height = 800) {\n  # Save the plot as an HTML file\n  saveWidget(sankey_plot, sprintf('%s/sankey.html',data_dir))\n}\n\nformat_scientific &lt;- function(x, digits=2) {\n  sapply(x, function(val) {\n    if (is.na(val) || abs(val) &lt; 1e-15) {\n      return(\"0\")\n    } else {\n      exponent &lt;- floor(log10(abs(val)))\n      coef &lt;- val / 10^exponent\n      #return(sprintf(\"%.1f × 10^%d\", round(coef, digits), exponent))\n      # May or may not be smart, just keeping magnitude\n      return(sprintf(\"10^%d\", exponent))\n    }\n  })\n}\n\ndata &lt;- result %&gt;% \n  mutate(across(c(genus_n_reads_tot, genus_ra_reads_tot), ~replace_na(., 0)),\n         genus = ifelse(is.na(genus), \"Unknown Genus\", genus)) %&gt;%\n  mutate(\n  species = paste0(species, sprintf(' (%s)', format_scientific(species_ra_reads_tot))),\n  genus = paste0(genus, sprintf(' (%s)', format_scientific(genus_ra_reads_tot))),\n  family = paste0(family, sprintf(' (%s)', format_scientific(family_ra_reads_tot)))\n)\nlinks &lt;- as.data.frame(create_links(data))\nnodes &lt;- create_nodes(links)\nsankey_data &lt;- prepare_sankey_data(links, nodes)\nsankey &lt;- create_sankey_plot(sankey_data)\n\nsankey\n\n\n\n\n\nTo get a better idea of the relative abundance of species, we can create a dot plot where each dot represents the relative abundance of a particular species in a sample.\n\nCodespecies_family &lt;- result %&gt;% select(species, family) %&gt;% rename('name' = 'species')\n\nplay &lt;- hv_species_counts %&gt;% \n  ungroup() %&gt;%\n  inner_join(libraries, by = 'sample') %&gt;%\n  inner_join(species_family, by = 'name')\n\nplay$family &lt;- factor(play$family, levels=hv_family_collate %&gt;% pull(family))\nplay &lt;- play %&gt;%\n  arrange(family, name) %&gt;%\n  mutate(name = factor(name, levels = unique(name)))\n#name_order &lt;- play %&gt;% arrange(family) %&gt;% pull(name)\n#play$name &lt;- factor(play$name, levels = name_order)\n\npal &lt;- c(brewer.pal(8, 'Dark2'),brewer.pal(8, 'Accent'))\n\nra_dot &lt;- ggplot(play, aes(x = ra_reads_hv, y=name)) +\n  geom_quasirandom(orientation = 'y', aes(color = family)) +\n  scale_color_manual(values = pal) + \n    scale_x_log10(\n    \"Relative abundance human virus reads\",\n    labels = function(x) parse(text = paste0(\"10^\", round(log10(x)))),\n    limits = c(1e-9, 1),\n    n.breaks = 8\n  ) +\n  labs(y = 'Human virus',\n       color = 'Viral family') + \n  theme_light() + \n    theme(\n    axis.text.y = element_text(size = 8),\n    axis.text.x = element_text(size = 14),\n    axis.ticks.y = element_blank(),axis.line = element_line(colour = \"black\"),\n    axis.title.x = element_text(size = 15),    \n    axis.title.y = element_text(size = 15),  \n    legend.text = element_text(size = 13),\n    legend.title = element_text(size = 16),\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank())\nra_dot\n\n\n\n\n\n\n\nWe now exclude Anelloviridae from the plot and add in the locations of the samples in a separate bar plot.\n\nCodeplay_subset &lt;- play %&gt;% filter(family != \"Anelloviridae\")\nplay_subset &lt;- play_subset%&gt;%\n  group_by(name) %&gt;%\n  mutate(virus_count = n_distinct(sample),\n         mean_ra = mean(ra_reads_hv)) %&gt;%\n  ungroup() %&gt;%\n  mutate(total_samples = n_distinct(libraries),\n         virus_prevalence = paste0(virus_count, \"/\", total_samples),\n         virus_prevalence_num = virus_count/total_samples,\n            name = paste0(name, ' (', virus_prevalence, ')')) %&gt;% \n  mutate(name = fct_reorder(name, virus_prevalence_num, .desc = TRUE))\n\nplay_subset &lt;- left_join(play_subset, libraries %&gt;% select(sample, location), by = 'sample')\n\n\npal &lt;- c(brewer.pal(8, 'Dark2'),brewer.pal(8, 'Accent'))\n\nra_dot &lt;- ggplot(play_subset, aes(x = ra_reads_hv, y=name)) +\n  geom_quasirandom(orientation = 'y', aes(color = family)) +\n#  geom_point(aes(x = mean_ra), shape = 18, size = 4) + \n  scale_color_manual(values = pal) + \n  scale_x_log10(\n    \"Relative abundance human virus reads\",\n    labels = function(x) parse(text = paste0(\"10^\", round(log10(x)))),\n    limits = c(1e-9, 1),\n    n.breaks = 8\n  ) +\n  labs(y = 'Human virus',\n       color = 'Viral family') +\n  theme_light() + \n  theme(\n    axis.text.y = element_text(size = 8),\n    axis.text.x = element_text(size = 14),\n    axis.ticks.y = element_blank(),axis.line = element_line(colour = \"black\"),\n    axis.title.x = element_text(size = 15),    \n    axis.title.y = element_text(size = 15),  \n    legend.text = element_text(size = 13),\n    legend.title = element_text(size = 16),\n    legend.position = c(1, 1),  # Move legend to top right\n    legend.justification = c(1, 1),  # Align legend to top right\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank())\n\nprevalence_bar &lt;- ggplot(play_subset, aes(x = virus_prevalence_num, y = name, fill = location)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_fill_brewer(palette = \"Pastel1\") +\n  scale_x_continuous(\n    \"Percent of samples with virus\",\n    labels = scales::percent_format(scale = 1, accuracy = 1)\n  ) +\n  labs(y = 'Human virus',\n       fill = 'Location') +\n  theme_light() + \n  theme(\n    axis.text.y = element_blank(),\n    axis.text.x = element_text(size = 14),\n    axis.ticks.y = element_blank(),\n    axis.line = element_line(colour = \"black\"),\n    axis.title.x = element_text(size = 15),    \n    axis.title.y = element_blank(),  \n    legend.text = element_text(size = 13),\n    legend.title = element_text(size = 16),\n    legend.position = c(1, 1),  # Move legend to top right\n    legend.justification = c(1, 1),  # Align legend to top right\n    panel.grid.minor = element_blank(),\n    panel.border = element_blank(),\n    panel.background = element_blank())\n\n# Combine the plots using ggarrange\ncombined_plot &lt;- ggarrange(ra_dot, prevalence_bar, \n                           ncol = 2, \n                           widths = c(1.2, 1),\n                           common.legend = FALSE,\n                           align = \"h\")\ncombined_plot\n\n\n\n\n\n\n\nI’d like to BLAST some of these reads against the NCBI nt database to see if we can get some more info on them."
  }
]