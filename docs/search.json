[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Harmon’s public notebook",
    "section": "",
    "text": "Workflow of Blauwkamp et al. (2019)\n\n\nIndividual plasma from USA\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nHarmon Bhasin\n\n\n\n\n\n\n\n\n\n\n\n\nWorkflow of Cebria-Mendoza et al. (2021)\n\n\nPooled plasma from Spain\n\n\n\n\n\n\n\n\nJul 8, 2024\n\n\nHarmon Bhasin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html",
    "href": "notebooks/2024-07-08_cebria-mendoza.html",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "",
    "text": "THIS IS CURRENTLY A WORK IN PROGRESS!\nAs a part of my time here, I’m exploring blood surveillance for a potential option for the NAO to pursue. Instead of writing a manuscript, we’ve decided to make three blog posts. The first blog post will cover why we’re considering blood, the second blog post will cover the blood industry, and the last blog post will look at the composition of blood.\nAfter exhaustively looking at a bunch of studies, we’ve decided upon two. This is the analysis of the first study. One of the steps in doing so is analyzing relative abundance data in the healthy population. In this post, I analyze Cebria-Mendoza 2021, a dataset with 60 samples from ~600 healthy blood donors in Spain."
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#high-level-metrics",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#high-level-metrics",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "High-level metrics",
    "text": "High-level metrics\nThe average fraction of reads at each stage in the preprocessing pipeline is shown in the following table. On average, cleaning & deduplication removed about 57% of total read pairs, primarily during duplication. Ribodepletion removed about 6-8% during each round.\n\nCode# Count read losses\nn_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% \n  arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, group=sample)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, \n                      aes(x=stage, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nTODO interpret the below plot.\n\nCodeg_qual &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\ng_adapters &lt;- g_qual + \n  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n                     breaks = seq(0,50,10), expand=c(0,0)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~adapter)\ng_adapters\n\n\n\n\n\n\nCode# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~., scales = \"free_y\")\ng_quality_seq\n\n\n\n\n\n\n\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#effectiveness-of-ribodepletion",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#effectiveness-of-ribodepletion",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Effectiveness of ribodepletion",
    "text": "Effectiveness of ribodepletion\nSome samples had anywhere as low as 10% all the way up to 60% ribosomal reads.\n\nCode# Calculate reads lost during ribodepletion (approximation for % ribosomal reads)\nreads_ribo &lt;- n_reads_rel %&gt;% \n  filter(stage %in% c(\"dedup\", \"ribo_secondary\")) %&gt;% \n  group_by(sample) %&gt;% \n  summarize(p_reads_ribo=1-n_read_pairs[2]/n_read_pairs[1], .groups = \"drop\") %&gt;%\n  inner_join(libraries)\n\nJoining with `by = join_by(sample)`\n\nCodereads_ribo_summ &lt;- reads_ribo %&gt;%\n  group_by(sample) %&gt;%\n  summarize(min=min(p_reads_ribo), max=max(p_reads_ribo),\n            mean=mean(p_reads_ribo), .groups = \"drop\") %&gt;%\n  inner_join(libraries)\n\nJoining with `by = join_by(sample)`\n\nCodeg_reads_ribo &lt;- ggplot(reads_ribo, \n                       aes(x=library, y=p_reads_ribo)) +\n  geom_point() + \n  scale_y_continuous(name=\"Approx % ribosomal reads\", limits=c(0,1),\n                     breaks=seq(0,1,0.2), expand=c(0,0), labels = function(y) y*100)+\n  theme_kit\ng_reads_ribo"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#high-level-composition",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#high-level-composition",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "High-level composition",
    "text": "High-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken2 and summarized the results with Bracken. This time, however, I used the full Standard database instead of Standard-16, and I also used a newer iteration of the database.\n\nCodeclassifications &lt;- c(\"Filtered\", \"Duplicate\", \"Ribosomal\", \"Unassigned\",\n                     \"Bacterial\", \"Archaeal\", \"Viral\", \"Human\")\n\n# Import composition data\ntax_final_dir &lt;- file.path(results_dir, \"taxonomy_final\")\ncomp_path &lt;- file.path(tax_final_dir, \"taxonomic_composition.tsv.gz\")\ncomp &lt;- read_tsv(comp_path) %&gt;% left_join(libraries) \n\nRows: 480 Columns: 4\n-- Column specification --------------------------------------------------------\nDelimiter: \"\\t\"\nchr (2): sample, classification\ndbl (2): n_reads, p_reads\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\nJoining with `by = join_by(sample)`\n\nCodecomp_minor &lt;- comp %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\ncomp_assigned &lt;- comp %&gt;%\n  filter(! classification %in% c(\"Filtered\", \"Duplicate\", \n                                 \"Ribosomal\", \"Unassigned\")) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads = n_reads/sum(n_reads))\ncomp_assigned_minor &lt;- comp_assigned %&gt;% \n  filter(classification %in% c(\"Archaeal\", \"Viral\", \"Human\", \"Other\"))\n\n# Summarize composition\nread_comp_summ &lt;- comp %&gt;% \n  group_by(classification) %&gt;%\n  summarize(n_reads = sum(n_reads), .groups = \"drop_last\") %&gt;%\n  mutate(n_reads = replace_na(n_reads,0),\n         p_reads = n_reads/sum(n_reads),\n         pc_reads = p_reads*100)\n\n\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=library, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Plasma pool\") +\n  theme_kit + \n  theme(plot.title = element_text(hjust=0, face=\"plain\", size=rel(1.5)))\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\ngeom_comp &lt;- purrr::partial(geom_col, position = \"stack\", width = 1)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_comp(data = comp) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, all groups)\")\ng_comp\n\n\n\n\n\n\nCode# Repeat for classified reads only\npalette_assigned &lt;- brewer.pal(9, \"Set1\")[5:9]\ng_comp_assigned &lt;- g_comp_base + \n  geom_comp(data = comp_assigned) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_manual(values=palette_assigned, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, all groups)\")\ng_comp_assigned\n\n\n\n\n\n\nCode# Plot composition of minor components\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_comp(data=comp_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, minor groups)\")\ng_comp_minor\n\n\n\n\n\n\nCodeg_comp_assigned_minor &lt;- g_comp_base + \n  geom_comp(data=comp_assigned_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, minor groups)\")\ng_comp_assigned_minor\n\n\n\n\n\n\n\nTODO analyze this above"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#total-viral-content",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#total-viral-content",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Total viral content",
    "text": "Total viral content\nTotal viral fraction average \\(1.81 \\times 10^{-2}\\) across samples. As a fraction of assigned (rather than total) reads, this jumped to \\(1.60 \\times {-1}\\):\n\nCodep_reads_viral_all &lt;- comp %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"All reads\")\np_reads_viral_assigned &lt;- comp_assigned %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"Classified reads\")\np_reads_viral &lt;- bind_rows(p_reads_viral_all, p_reads_viral_assigned)\n\n# Plot\ng_viral &lt;- ggplot(p_reads_viral, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_x_discrete(name=\"Plasma pool\") +\n  scale_y_log10(name=\"Viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  guides(color=guide_legend(nrow=2), shape=guide_legend(nrow=2),\n         linetype=guide_legend(nrow=2)) +\n  theme_kit\ng_viral"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#taxonomic-composition-of-viruses",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#taxonomic-composition-of-viruses",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Taxonomic composition of viruses",
    "text": "Taxonomic composition of viruses\nThe two dominant viruses we see are Anellovirdae and Rhabdovirdae. Followed by these two viral families is Flavivirdae, and lastly, also by a much smaller percent, Microviridae.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"total-virus-db.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get Kraken reports\nreports_path &lt;- file.path(tax_final_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% arrange(sample)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %&gt;% ungroup\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.01\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries)\n\nJoining with `by = join_by(sample)`\n\nCode# Plot\ng_families &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_brewer(palette = 'Accent')\ng_families\n\n\n\n\n\n\n\nExcluding Anellovirdae and Rhabdovirdae, remaining viral sequences are distributed across a wide variety:\n\nCodemajor_threshold_adj &lt;- 0.05\n\n# Adjust viral family counts\nviral_families_adj &lt;- viral_families %&gt;%\n  filter(!(name %in% c(\"Rhabdoviridae\",\"Anelloviridae\"))) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = p_reads_viral/sum(p_reads_viral))\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families_adj %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families_adj %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries)\n\nJoining with `by = join_by(sample)`\n\nCode# Plot\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"), brewer.pal(9, \"Set1\"))\ng_families_adj &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\ng_families_adj"
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#overall-relative-abundance",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#overall-relative-abundance",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Overall relative abundance",
    "text": "Overall relative abundance\nI calculated the relative abundance of human-infecting viruses in two ways:\n\nFirst, as the total number of deduplicated human-virus reads in each sample, divided by the number of raw reads (“All reads”).\nSecond, as a fraction of preprocessed (cleaned, deduplicated, computationally ribodepleted) reads (“Preprocessed reads”).\n\n\nCode# Import and format reads\nhv_reads_path &lt;- file.path(hv_dir, \"hv_hits_putative_collapsed.tsv.gz\")\nmrg_hv &lt;- read_tsv(hv_reads_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% arrange(sample) %&gt;%\n  mutate(kraken_label = ifelse(assigned_hv, \"Kraken2 HV assignment\",\n                               \"No Kraken2 assignment\")) %&gt;%\n  mutate(adj_score_max = pmax(adj_score_fwd, adj_score_rev),\n         highscore = adj_score_max &gt;= 20,\n         hv_status = assigned_hv | highscore) %&gt;%\n  rename(taxid_all = taxid, taxid = taxid_best)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\nCode# Get raw read counts\nread_counts_raw &lt;- filter(basic_stats_raw) %&gt;%\n  select(sample, n_reads_raw = n_read_pairs)\nread_counts_preproc &lt;- basic_stats %&gt;% filter(stage == \"ribo_initial\") %&gt;%\n  select(sample, n_reads_preproc = n_read_pairs)\n\n# Get HV read counts\nread_counts_hv &lt;- mrg_hv %&gt;% filter(hv_status) %&gt;% \n  group_by(sample) %&gt;% \n  count(name=\"n_reads_hv\")\nread_counts &lt;- read_counts_raw %&gt;%\n  left_join(read_counts_hv, by=c(\"sample\")) %&gt;%\n  mutate(n_reads_hv = replace_na(n_reads_hv, 0)) %&gt;%\n  left_join(read_counts_preproc, by=c(\"sample\")) %&gt;%\n  inner_join(libraries, by=c(\"sample\")) %&gt;%\n  select(sample, n_reads_raw, n_reads_preproc, n_reads_hv) %&gt;%\n  mutate(n_samples = 1,\n         p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc)\nread_counts_long &lt;- read_counts %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\"))\n\n# Combine for display\nread_counts_agg &lt;- read_counts %&gt;%\n  mutate(p_reads_total = n_reads_hv/n_reads_raw,\n         p_reads_preproc = n_reads_hv/n_reads_preproc) %&gt;%\n  inner_join(libraries)\nread_counts_agg_long &lt;- read_counts_agg %&gt;%\n  pivot_longer(starts_with(\"p_reads\"), names_to=\"read_group\", values_to=\"p_reads\") %&gt;%\n  mutate(read_group = ifelse(read_group == \"p_reads_total\", \"All reads\", \"Preprocessed reads\"))%&gt;%\n  inner_join(libraries)\n\n# Visualize\ng_read_counts &lt;- ggplot(read_counts_agg_long, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_y_log10(name = \"Unique human-viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  theme_kit\ng_read_counts\n\n\n\n\n\n\n\nTODO do analysis above."
  },
  {
    "objectID": "notebooks/2024-07-08_cebria-mendoza.html#overall-taxonomy-and-composition",
    "href": "notebooks/2024-07-08_cebria-mendoza.html#overall-taxonomy-and-composition",
    "title": "Workflow of Cebria-Mendoza et al. (2021)",
    "section": "Overall taxonomy and composition",
    "text": "Overall taxonomy and composition\nComposition of HV reads was not greatly changed from when looking at all viral reads. The two dominant viruses we see are Anellovirdae and Rhabdovirdae. Followed by these two viral families is Flavivirdae, and lastly, also by a much smaller percent, Microviridae.\n\nCode# Filter samples and add viral taxa information\nsamples_keep &lt;- read_counts %&gt;% filter(n_reads_hv &gt; 5) %&gt;% pull(sample)\nmrg_hv_named &lt;- mrg_hv %&gt;% filter(sample %in% samples_keep, hv_status) %&gt;% left_join(viral_taxa, by=\"taxid\") \n\n# Discover viral species & genera for HV reads\nraise_rank &lt;- function(read_db, taxid_db, out_rank = \"species\", verbose = FALSE){\n  # Get higher ranks than search rank\n  ranks &lt;- c(\"subspecies\", \"species\", \"subgenus\", \"genus\", \"subfamily\", \"family\", \"suborder\", \"order\", \"class\", \"subphylum\", \"phylum\", \"kingdom\", \"superkingdom\")\n  rank_match &lt;- which.max(ranks == out_rank)\n  high_ranks &lt;- ranks[rank_match:length(ranks)]\n  # Merge read DB and taxid DB\n  reads &lt;- read_db %&gt;% select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  # Extract sequences that are already at appropriate rank\n  reads_rank &lt;- filter(reads, rank == out_rank)\n  # Drop sequences at a higher rank and return unclassified sequences\n  reads_norank &lt;- reads %&gt;% filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  while(nrow(reads_norank) &gt; 0){ # As long as there are unclassified sequences...\n    # Promote read taxids and re-merge with taxid DB, then re-classify and filter\n    reads_remaining &lt;- reads_norank %&gt;% mutate(taxid = parent_taxid) %&gt;%\n      select(-parent_taxid, -rank, -name) %&gt;%\n      left_join(taxid_db, by=\"taxid\")\n    reads_rank &lt;- reads_remaining %&gt;% filter(rank == out_rank) %&gt;%\n      bind_rows(reads_rank)\n    reads_norank &lt;- reads_remaining %&gt;%\n      filter(rank != out_rank, !rank %in% high_ranks, !is.na(taxid))\n  }\n  # Finally, extract and append reads that were excluded during the process\n  reads_dropped &lt;- reads %&gt;% filter(!seq_id %in% reads_rank$seq_id)\n  reads_out &lt;- reads_rank %&gt;% bind_rows(reads_dropped) %&gt;%\n    select(-parent_taxid, -rank, -name) %&gt;%\n    left_join(taxid_db, by=\"taxid\")\n  return(reads_out)\n}\nhv_reads_species &lt;- raise_rank(mrg_hv_named, viral_taxa, \"species\")\nhv_reads_genus &lt;- raise_rank(mrg_hv_named, viral_taxa, \"genus\")\nhv_reads_family &lt;- raise_rank(mrg_hv_named, viral_taxa, \"family\")\n\n\n\nCodethreshold_major_family &lt;- 0.05\n\n# Count reads for each human-viral family\nhv_family_counts &lt;- hv_reads_family %&gt;% \n  group_by(sample, name, taxid) %&gt;%\n  count(name = \"n_reads_hv\") %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_hv = n_reads_hv/sum(n_reads_hv))\n\n# Identify high-ranking families and group others\nhv_family_major_tab &lt;- hv_family_counts %&gt;% group_by(name) %&gt;% \n  filter(p_reads_hv == max(p_reads_hv)) %&gt;% filter(row_number() == 1) %&gt;%\n  arrange(desc(p_reads_hv)) %&gt;% filter(p_reads_hv &gt; threshold_major_family)\nhv_family_counts_major &lt;- hv_family_counts %&gt;%\n  mutate(name_display = ifelse(name %in% hv_family_major_tab$name, name, \"Other\")) %&gt;%\n  group_by(sample, name_display) %&gt;%\n  summarize(n_reads_hv = sum(n_reads_hv), p_reads_hv = sum(p_reads_hv), \n            .groups=\"drop\") %&gt;%\n  mutate(name_display = factor(name_display, \n                               levels = c(hv_family_major_tab$name, \"Other\")))\nhv_family_counts_display &lt;- hv_family_counts_major %&gt;%\n  rename(p_reads = p_reads_hv, classification = name_display) %&gt;%\n  inner_join(libraries)\n\nJoining with `by = join_by(sample)`\n\nCode# Plot\ng_hv_family &lt;- g_comp_base + \n  geom_col(data=hv_family_counts_display, position = \"stack\", width=1) +\n  scale_y_continuous(name=\"% HV Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_brewer(palette = 'Accent', name = \"Viral family\") +\n  labs(title=\"Family composition of human-viral reads\") +\n  guides(fill=guide_legend(ncol=4)) +\n  theme(plot.title = element_text(size=rel(1.4), hjust=0, face=\"plain\"))\ng_hv_family\n\n\n\n\n\n\nCode# Get most prominent families for text\nhv_family_collate &lt;- hv_family_counts %&gt;%\n  group_by(name, taxid) %&gt;% \n  summarize(n_reads_tot = sum(n_reads_hv),\n            p_reads_max = max(p_reads_hv), .groups=\"drop\") %&gt;% \n  arrange(desc(n_reads_tot))\nhv_family_collate\n\n\n  \n\n\n\n\nCode#test &lt;- hv_reads_family %&gt;% filter(hit_hv == 1 & name %in% c('Anelloviridae', 'Flaviviridae', 'Hepeviridae', 'Microviridae', 'Rhabdoviridae')) %&gt;% select(name, assigned_name) %&gt;% distinct()\n#hv_reads_family %&gt;% filter(hit_hv == 1 & name %in% c('Flaviviridae', 'Hepeviridae', 'Microviridae', 'Rhabdoviridae')) %&gt;% select(name, assigned_name) %&gt;% distinct()\n\n#hv_reads_family %&gt;% filter(hit_hv == 1) %&gt;% select(name, assigned_name) %&gt;% distinct() %&gt;% filter(!str_detect(tolower(assigned_name), \"torque teno\"))\nhv_reads_family %&gt;% select(name, assigned_name) %&gt;% distinct() %&gt;% filter(!str_detect(tolower(assigned_name), \"torque teno\"))\n\n\n  \n\n\nCodehv_reads_family %&gt;% select(assigned_name) %&gt;% distinct() %&gt;% filter(!str_detect(tolower(assigned_name), \"torque teno\"))\n\n\n  \n\n\nCodehv_reads_genus %&gt;% select(name) %&gt;% distinct() %&gt;% filter(!str_detect(tolower(name), \"torque teno\"))\n\n\n  \n\n\n\nUnsurprisingly we get a lot of Anello virus, specifically torque teno virus. Once I filter those out we get a few interesting viruses: - Rhabdovirus - Vesiculovirus - Vesicular stomatitis Indiana virus - Maraba virus - Flavivirus - Pegivirus - GB virus C\nTODO look in to this."
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html",
    "href": "notebooks/2024-07-09_blauwkamp.html",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "",
    "text": "THIS IS CURRENTLY A WORK IN PROGRESS!\nThis is the second study of this series. In this post, I analyze Blauwkamp 2019, a dataset with ~170 samples, one for each individual of cell-free DNA in plasma in the United States."
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#high-level-metrics",
    "href": "notebooks/2024-07-09_blauwkamp.html#high-level-metrics",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "High-level metrics",
    "text": "High-level metrics\nThe average fraction of reads at each stage in the preprocessing pipeline is shown in the following table. On average, cleaning & deduplication removed about X% of total read pairs, primarily during duplication. Ribodepletion removed about X% during each round.\n\nCode# Count read losses\nn_reads_rel &lt;- basic_stats %&gt;% \n  select(sample, stage, percent_duplicates, n_read_pairs) %&gt;%\n  group_by(sample) %&gt;% \n  arrange(sample, stage) %&gt;%\n  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),\n         p_reads_lost = 1 - p_reads_retained,\n         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],\n         p_reads_lost_abs = 1-p_reads_retained_abs,\n         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))\nn_reads_rel_display &lt;- n_reads_rel %&gt;% \n  rename(Stage=stage) %&gt;% \n  group_by(Stage) %&gt;% \n  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), \"-\", round(max(p_reads_lost_abs*100),1), \" (mean \", round(mean(p_reads_lost_abs*100),1), \")\"),\n            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), \"-\", round(max(p_reads_lost_abs_marginal*100),1), \" (mean \", round(mean(p_reads_lost_abs_marginal*100),1), \")\"), .groups=\"drop\") %&gt;% \n  filter(Stage != \"raw_concat\") %&gt;%\n  mutate(Stage = Stage %&gt;% as.numeric %&gt;% factor(labels=c(\"Trimming & filtering\", \"Deduplication\", \"Initial ribodepletion\", \"Secondary ribodepletion\")))\nn_reads_rel_display\n\n\n  \n\n\n\n\nCodeg_stage_trace &lt;- ggplot(basic_stats, aes(x=stage, group=sample)) +\n  theme_kit\n\n# Plot reads over preprocessing\ng_reads_stages &lt;- g_stage_trace +\n  geom_line(aes(y=n_read_pairs)) +\n  scale_y_continuous(\"# Read pairs\", expand=c(0,0), limits=c(0,NA))\ng_reads_stages\n\n\n\n\n\n\nCode# Plot relative read losses during preprocessing\ng_reads_rel &lt;- ggplot(n_reads_rel, \n                      aes(x=stage, group=sample)) +\n  geom_line(aes(y=p_reads_lost_abs_marginal)) +\n  scale_y_continuous(\"% Total Reads Lost\", expand=c(0,0), \n                     labels = function(x) x*100) +\n  theme_kit\ng_reads_rel\n\n\n\n\n\n\n\nTODO interpret the below plot.\n\nCodeg_qual &lt;- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + \n  scale_linetype_discrete(name = \"Read Pair\") +\n  guides(color=guide_legend(nrow=2,byrow=TRUE),\n         linetype = guide_legend(nrow=2,byrow=TRUE)) +\n  theme_base\n\n# Visualize adapters\n#g_adapters &lt;- g_qual + \n#  geom_line(aes(x=position, y=pc_adapters), data=adapter_stats) +\n#  scale_y_continuous(name=\"% Adapters\", limits=c(0,20),\n#                     breaks = seq(0,50,10), expand=c(0,0)) +\n#  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n#                     breaks=seq(0,140,20), expand=c(0,0)) +\n#  facet_grid(stage~adapter)\n#g_adapters\n\n# Visualize quality\ng_quality_base &lt;- g_qual +\n  geom_hline(yintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_hline(yintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +\n  scale_y_continuous(name=\"Mean Phred score\", expand=c(0,0), limits=c(10,45)) +\n  scale_x_continuous(name=\"Position\", limits=c(0,NA),\n                     breaks=seq(0,140,20), expand=c(0,0)) +\n  facet_grid(stage~.)\ng_quality_base\n\n\n\n\n\n\nCodeg_quality_seq &lt;- g_qual +\n  geom_vline(xintercept=25, linetype=\"dashed\", color=\"red\") +\n  geom_vline(xintercept=30, linetype=\"dashed\", color=\"red\") +\n  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +\n  scale_x_continuous(name=\"Mean Phred score\", expand=c(0,0)) +\n  scale_y_continuous(name=\"# Sequences\", expand=c(0,0)) +\n  facet_grid(stage~., scales = \"free_y\")\ng_quality_seq\n\n\n\n\n\n\n\n\nCodestage_dup &lt;- basic_stats %&gt;% group_by(stage) %&gt;% \n  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),\n            dmean=mean(percent_duplicates), .groups = \"drop\")\n\ng_dup_stages &lt;- g_stage_trace +\n  geom_line(aes(y=percent_duplicates)) +\n  scale_y_continuous(\"% Duplicates\", limits=c(0,NA), expand=c(0,0))\ng_dup_stages\n\n\n\n\n\n\nCodeg_readlen_stages &lt;- g_stage_trace + geom_line(aes(y=mean_seq_len)) +\n  scale_y_continuous(\"Mean read length (nt)\", expand=c(0,0), limits=c(0,NA))\ng_readlen_stages"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#effectiveness-of-ribodepletion",
    "href": "notebooks/2024-07-09_blauwkamp.html#effectiveness-of-ribodepletion",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "Effectiveness of ribodepletion",
    "text": "Effectiveness of ribodepletion\n\nCode# Calculate reads lost during ribodepletion (approximation for % ribosomal reads)\nreads_ribo &lt;- n_reads_rel %&gt;% \n  filter(stage %in% c(\"dedup\", \"ribo_secondary\")) %&gt;% \n  group_by(sample) %&gt;% \n  summarize(p_reads_ribo=1-n_read_pairs[2]/n_read_pairs[1], .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\nreads_ribo_summ &lt;- reads_ribo %&gt;%\n  group_by(sample) %&gt;%\n  summarize(min=min(p_reads_ribo), max=max(p_reads_ribo),\n            mean=mean(p_reads_ribo), .groups = \"drop\") %&gt;%\n  inner_join(libraries, by = 'sample')\ng_reads_ribo &lt;- ggplot(reads_ribo, \n                       aes(x=library, y=p_reads_ribo)) +\n  geom_point() + \n  scale_y_continuous(name=\"Approx % ribosomal reads\", limits=c(0,1),\n                     breaks=seq(0,1,0.2), expand=c(0,0), labels = function(y) y*100)+\n  theme_kit\ng_reads_ribo"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#high-level-composition",
    "href": "notebooks/2024-07-09_blauwkamp.html#high-level-composition",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "High-level composition",
    "text": "High-level composition\nAs before, to assess the high-level composition of the reads, I ran the ribodepleted files through Kraken2 and summarized the results with Bracken. This time, however, I used the full Standard database instead of Standard-16, and I also used a newer iteration of the database.\n\nCode# Prepare plotting templates\ng_comp_base &lt;- ggplot(mapping=aes(x=library, y=p_reads, fill=classification)) +\n  scale_x_discrete(name=\"Plasma pool\") +\n  theme_kit + \n  theme(plot.title = element_text(hjust=0, face=\"plain\", size=rel(1.5)))\nscale_y_pc_reads &lt;- purrr::partial(scale_y_continuous, name = \"% Reads\",\n                                   expand = c(0,0), labels = function(y) y*100)\ngeom_comp &lt;- purrr::partial(geom_col, position = \"stack\", width = 1)\n\n# Plot overall composition\ng_comp &lt;- g_comp_base + geom_comp(data = comp) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_brewer(palette = \"Set1\", name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, all groups)\")\ng_comp\n\n\n\n\n\n\nCode# Repeat for classified reads only\npalette_assigned &lt;- brewer.pal(9, \"Set1\")[5:9]\ng_comp_assigned &lt;- g_comp_base + \n  geom_comp(data = comp_assigned) +\n  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +\n  scale_fill_manual(values=palette_assigned, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, all groups)\")\ng_comp_assigned\n\n\n\n\n\n\nCode# Plot composition of minor components\npalette_minor &lt;- brewer.pal(9, \"Set1\")[6:9]\ng_comp_minor &lt;- g_comp_base + \n  geom_comp(data=comp_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (all reads, minor groups)\")\ng_comp_minor\n\n\n\n\n\n\nCodeg_comp_assigned_minor &lt;- g_comp_base + \n  geom_comp(data=comp_assigned_minor) +\n  scale_y_pc_reads() +\n  scale_fill_manual(values=palette_minor, name = \"Classification\") +\n  ggtitle(\"Read composition (assigned reads, minor groups)\")\ng_comp_assigned_minor\n\n\n\n\n\n\n\nTODO analyze this above"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#total-viral-content",
    "href": "notebooks/2024-07-09_blauwkamp.html#total-viral-content",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "Total viral content",
    "text": "Total viral content\nTotal viral fraction average \\(x\\) across samples. As a fraction of assigned (rather than total) reads, this jumped to \\(y\\):\n\nCodep_reads_viral_all &lt;- comp %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"All reads\")\np_reads_viral_assigned &lt;- comp_assigned %&gt;% filter(classification == \"Viral\") %&gt;%\n  mutate(read_group = \"Classified reads\")\np_reads_viral &lt;- bind_rows(p_reads_viral_all, p_reads_viral_assigned)\n\n# Plot\ng_viral &lt;- ggplot(p_reads_viral, aes(x=library, y=p_reads)) +\n  geom_point() +\n  scale_x_discrete(name=\"Plasma pool\") +\n  scale_y_log10(name=\"Viral read fraction\") +\n  facet_grid(.~read_group, scales = \"free\") +\n  guides(color=guide_legend(nrow=2), shape=guide_legend(nrow=2),\n         linetype=guide_legend(nrow=2)) +\n  theme_kit\ng_viral"
  },
  {
    "objectID": "notebooks/2024-07-09_blauwkamp.html#taxonomic-composition-of-viruses",
    "href": "notebooks/2024-07-09_blauwkamp.html#taxonomic-composition-of-viruses",
    "title": "Workflow of Blauwkamp et al. (2019)",
    "section": "Taxonomic composition of viruses",
    "text": "Taxonomic composition of viruses\nWe see a lot of viruses, not sure how to interpret this lol.\n\nCode# Get viral taxonomy\nviral_taxa_path &lt;- file.path(data_dir, \"total-virus-db.tsv.gz\")\nviral_taxa &lt;- read_tsv(viral_taxa_path, show_col_types = FALSE)\n\n# Get Kraken reports\nreports_path &lt;- file.path(tax_final_dir, \"kraken_reports.tsv.gz\")\nreports &lt;- read_tsv(reports_path, show_col_types = FALSE) %&gt;%\n  inner_join(libraries, by=\"sample\") %&gt;% arrange(sample)\n\n# Filter to viral taxa\nkraken_reports_viral &lt;- filter(reports, taxid %in% viral_taxa$taxid) %&gt;%\n  group_by(sample) %&gt;%\n  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])\nkraken_reports_viral_cleaned &lt;- kraken_reports_viral %&gt;%\n  select(-pc_reads_total, -n_reads_direct, -contains(\"minimizers\")) %&gt;%\n  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %&gt;% ungroup\n\nviral_classes &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"C\")\nviral_families &lt;- kraken_reports_viral_cleaned %&gt;% filter(rank == \"F\")\n\n\n\nCodemajor_threshold &lt;- 0.01\n\n# Identify major viral families\nviral_families_major_tab &lt;- viral_families %&gt;% \n  group_by(name, taxid) %&gt;%\n  summarize(p_reads_viral_max = max(p_reads_viral), .groups=\"drop\") %&gt;%\n  filter(p_reads_viral_max &gt;= major_threshold)\nviral_families_major_list &lt;- viral_families_major_tab %&gt;% pull(name)\nviral_families_major &lt;- viral_families %&gt;% \n  filter(name %in% viral_families_major_list) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_minor &lt;- viral_families_major %&gt;% \n  group_by(sample) %&gt;%\n  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = \"drop\") %&gt;%\n  mutate(name = \"Other\", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %&gt;%\n  select(name, taxid, sample, p_reads_viral)\nviral_families_display &lt;- viral_families_major %&gt;% \n  bind_rows(viral_families_minor) %&gt;%\n  arrange(desc(p_reads_viral)) %&gt;% \n  mutate(name = factor(name, levels=c(viral_families_major_list, \"Other\"))) %&gt;%\n  rename(p_reads = p_reads_viral, classification=name) %&gt;%\n  inner_join(libraries, by='sample')\n\n# Plot\npalette_viral &lt;- c(brewer.pal(12, \"Set3\"), brewer.pal(8, \"Dark2\"))\ng_families &lt;- g_comp_base + \n  geom_comp(data=viral_families_display) +\n  scale_y_continuous(name=\"% Viral Reads\", limits=c(0,1.01), \n                     breaks = seq(0,1,0.2),\n                     expand=c(0,0), labels = function(y) y*100) +\n  scale_fill_manual(values=palette_viral, name = \"Viral class\")\ng_families"
  }
]