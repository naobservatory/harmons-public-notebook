---
title: "Workflow of Blauwkamp et al. (2019)"
subtitle: "Individual plasma from USA"
author: "Harmon Bhasin"
date: 2024-07-09
format:
  html:
    code-fold: true
    code-tools: true
    code-link: true
    df-print: paged
execute: 
  freeze: auto
editor: visual
title-block-banner: "#de2d26" 
---

```{r}
#| label: load-packages
#| include: false
library(tidyverse)
library(cowplot)
library(patchwork)
library(fastqcr)
library(RColorBrewer)
library(networkD3)
source("/Users/harmonbhasin/work/securebio/sampling-strategies/scripts/aux_plot-theme.R")

theme_base <- theme_base + theme(aspect.ratio = NULL)
theme_kit <- theme_base + theme(
  axis.text.x = element_text(hjust = 1, angle = 45),
  axis.title.x = element_blank(),
)
tnl <- theme(legend.position = "none")
```

THIS IS CURRENTLY A WORK IN PROGRESS!

This is the second study of [this series](https://data.securebio.org/harmons-public-notebook/notebooks/2024-07-08_cebria-mendoza.html). In this post, I analyze [Blauwkamp 2019](https://doi.org/10.1038/s41564-018-0349-6), a dataset with \~170 samples, one for each individual of cell-free DNA in plasma in the United States.

# The raw data

The samples utilized in the determination of the reference range were collected from 167 healthy asymptomatic donors in five geographically diverse areas of the United States who were 18 to 65 years of age and had been screened for common health conditions including infectious diseases through a questionnaire and standard blood donor screening assays (Serologix and StemExpress). We only have DNA sequencing for this data.

In total, these 170 samples contained 115M read pairs. The samples had 8 - 2M (mean .7M) read pairs each.

COME BACK ADAPTER STATISTICS WERE NOT COMPUTED, slightly fucked up the multiqc, but we can solve it here. This probably explains why adapter statsistics were not computed.

```{r}
#| warning: false
#| label: read-qc-data
#| include: false

# Data input paths
data_dir <- "/Users/harmonbhasin/work/securebio/nao-harmon/blauwkamp2019/analysis/"
input_dir <- file.path(data_dir, "input")
results_dir <- file.path(data_dir, "results")
qc_dir <- file.path(results_dir, "qc")
#hv_dir <- file.path(results_dir, "hv")
libraries_path <- file.path(input_dir, "libraries.csv")
basic_stats_path <- file.path(qc_dir, "qc_basic_stats.tsv.gz")
adapter_stats_path <- file.path(qc_dir, "qc_adapter_stats.tsv.gz")
quality_base_stats_path <- file.path(qc_dir, "qc_quality_base_stats.tsv.gz")
quality_seq_stats_path <- file.path(qc_dir, "qc_quality_sequence_stats.tsv.gz")

# Import libraries and extract metadata from sample names
libraries <- read_csv(libraries_path, show_col_types = FALSE)
#meta_data <- read_csv(sprintf('%s/SraRunTable.txt', data_dir)) %>%
#  rename(library=Run)
#libraries <- left_join(libraries_raw, meta_data) %>%
#  select(
#    library,
#    AvgSpotLen,
#    `Library Name`
#  ) %>%
#  rename(sample=library,
#    library=`Library Name`)

#libraries$library <- factor(libraries$library, labels=c(1:170))

# Import QC data
stages <- c("raw_concat", "cleaned", "dedup", "ribo_initial", "ribo_secondary")
replace_words <- c("_fastp", "_cleaned", "_dedup", "_bbduk_pass", "_ribo_initial", "_ribo_secondary")

pre_basic_stats <- read_tsv(basic_stats_path, show_col_types = FALSE) %>%
  mutate(sample = str_replace_all(sample, 
                                  paste(replace_words, collapse = "|"), 
                                  ""))

most_to_least_reads_sample_list <- pre_basic_stats %>% filter(stage == 'raw_concat') %>% arrange(desc(n_read_pairs)) %>% pull(sample)
libraries$library <- factor(libraries$library, levels=most_to_least_reads_sample_list, labels=c(1:170))

pre_basic_stats <- pre_basic_stats %>%
  inner_join(libraries, by="sample") %>% arrange(sample) %>%
  mutate(stage = factor(stage, levels = stages),
         sample = fct_inorder(sample))

pre_quality_base_stats <- read_tsv(quality_base_stats_path, show_col_types = FALSE) %>%
    mutate(sample = str_replace_all(sample, 
                                  paste(replace_words, collapse = "|"), 
                                  "")) %>%
  inner_join(libraries, by="sample") %>% arrange(sample) %>%
  mutate(stage = factor(stage, levels = stages),
         read_pair = fct_inorder(as.character(read_pair)))
  
pre_quality_seq_stats <- read_tsv(quality_seq_stats_path, show_col_types = FALSE) %>%
    mutate(sample = str_replace_all(sample, 
                                  paste(replace_words, collapse = "|"), 
                                  "")) %>%
  inner_join(libraries, by="sample") %>% arrange(sample) %>%
  mutate(stage = factor(stage, levels = stages),
         read_pair = fct_inorder(as.character(read_pair)))
  
  
 # Filter to raw data
 pre_basic_stats_raw <- pre_basic_stats %>% filter(stage == "raw_concat")
 pre_quality_base_stats_raw <- pre_quality_base_stats %>% filter(stage == "raw_concat")
 pre_quality_seq_stats_raw <- pre_quality_seq_stats %>% filter(stage == "raw_concat")
 
 
 # Get key values for readout
pre_raw_read_counts <- pre_basic_stats_raw %>% 
   summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),
             rmean=mean(n_read_pairs), 
             rtot = sum(n_read_pairs),
             btot = sum(n_bases_approx),
             dmin = min(percent_duplicates), dmax=max(percent_duplicates),
             dmean=mean(percent_duplicates), .groups = "drop")
```

You may have noticed that the dataset has 170 samples, however they only report 167 samples in the paper. This is because three of the samples have very few reads, for this reason, we omit those, when we do that we get the following statistics. We then go from 170 samples to 167 samples containing approximately for both 115M read pairs. The samples went from 8 - 2M (mean .7M) read pairs each to .1M-2M (mean .7M).

```{r}
#| warning: false
#| label: adjust-qc-data
#| include: true

remove_samples <- pre_basic_stats %>% filter(n_read_pairs < 100) %>% select(sample) %>% distinct() %>% pull()

# Filter libraries
libraries <- libraries %>%
  filter(!(sample %in% remove_samples)) 

# Import QC data
basic_stats <- pre_basic_stats %>%
  filter(!(sample %in% remove_samples))

quality_base_stats <- pre_quality_base_stats %>%
  filter(!(sample %in% remove_samples))
  
quality_seq_stats <- pre_quality_seq_stats %>%
  filter(!(sample %in% remove_samples))
  
 # Filter to raw data
 basic_stats_raw <- basic_stats %>% filter(stage == "raw_concat")
 quality_base_stats_raw <- quality_base_stats %>% filter(stage == "raw_concat")
 quality_seq_stats_raw <- quality_seq_stats %>% filter(stage == "raw_concat")
 
# Get key values for readout
raw_read_counts <- basic_stats_raw %>% 
   summarize(rmin = min(n_read_pairs), rmax=max(n_read_pairs),
             rmean=mean(n_read_pairs), 
             rtot = sum(n_read_pairs),
             btot = sum(n_bases_approx),
             dmin = min(percent_duplicates), dmax=max(percent_duplicates),
             dmean=mean(percent_duplicates), .groups = "drop")
```

```{r}
#| fig-width: 25
#| warning: false
#| label: plot-basic-stats

# Prepare data
basic_stats_raw_metrics <- basic_stats_raw %>%
  select(library,
         `# Reads` = n_read_pairs,
         `Total base pairs\n(approx)` = n_bases_approx,
         `% Duplicates\n(FASTQC)` = percent_duplicates) %>%
  pivot_longer(-library, names_to = "metric", values_to = "value") %>%
  mutate(metric = fct_inorder(metric))

g_basic <- ggplot(basic_stats_raw_metrics, aes(x=library, y=value)) +
  geom_col(position = "dodge") +
  scale_x_discrete() +
  scale_y_continuous(expand=c(0,0)) +
  expand_limits(y=c(0,100)) +
  facet_grid(metric~., scales = "free", space="free_x", switch="y") +
  theme_kit + theme(
    axis.title.y = element_blank(),
    strip.text.y = element_text(face="plain")
  )
g_basic
```

TODO interpret the below plot.

```{r}
#| label: plot-raw-quality
#| fig-width: 8

# Set up plotting templates
g_qual_raw <- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + 
  scale_linetype_discrete(name = "Read") +
  theme_base

# Visualize quality
g_quality_base_raw <- g_qual_raw +
  geom_hline(yintercept=25, linetype="dashed", color="red") +
  geom_hline(yintercept=30, linetype="dashed", color="red") +
  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats_raw) +
  scale_y_continuous(name="Mean Phred score", expand=c(0,0), limits=c(10,45)) +
  scale_x_continuous(name="Position", limits=c(0,NA),
                     breaks=seq(0,500,20), expand=c(0,0))
g_quality_base_raw

g_quality_seq_raw <- g_qual_raw +
  geom_vline(xintercept=25, linetype="dashed", color="red") +
  geom_vline(xintercept=30, linetype="dashed", color="red") +
  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats_raw) +
  scale_x_continuous(name="Mean Phred score", expand=c(0,0)) +
  scale_y_continuous(name="# Sequences", expand=c(0,0))
g_quality_seq_raw
```

# Preprocessing

## High-level metrics

The average fraction of reads at each stage in the preprocessing pipeline is shown in the following table. Given that human reads were removed before hand, we'd expect our trimming and filtering to make minimal change, and that's what we see. However, the researchers did not deduplicate and we can see that that removes a large portion of our reads, about 25% on average. Ribdepletion doesn't remove any reads, I'm not sure how to feel about this.

```{r}
#| label: preproc-table

# Count read losses
n_reads_rel <- basic_stats %>% 
  select(sample, stage, percent_duplicates, n_read_pairs) %>%
  group_by(sample) %>% 
  arrange(sample, stage) %>%
  mutate(p_reads_retained = n_read_pairs / lag(n_read_pairs),
         p_reads_lost = 1 - p_reads_retained,
         p_reads_retained_abs = n_read_pairs / n_read_pairs[1],
         p_reads_lost_abs = 1-p_reads_retained_abs,
         p_reads_lost_abs_marginal = p_reads_lost_abs - lag(p_reads_lost_abs))
n_reads_rel_display <- n_reads_rel %>% 
  rename(Stage=stage) %>% 
  group_by(Stage) %>% 
  summarize(`% Total Reads Lost (Cumulative)` = paste0(round(min(p_reads_lost_abs*100),1), "-", round(max(p_reads_lost_abs*100),1), " (mean ", round(mean(p_reads_lost_abs*100),1), ")"),
            `% Total Reads Lost (Marginal)` = paste0(round(min(p_reads_lost_abs_marginal*100),1), "-", round(max(p_reads_lost_abs_marginal*100),1), " (mean ", round(mean(p_reads_lost_abs_marginal*100),1), ")"), .groups="drop") %>% 
  filter(Stage != "raw_concat") %>%
  mutate(Stage = Stage %>% as.numeric %>% factor(labels=c("Trimming & filtering", "Deduplication", "Initial ribodepletion", "Secondary ribodepletion")))
n_reads_rel_display
```

```{r}
#| label: preproc-figures
#| warning: false
#| fig-height: 3
#| fig-width: 6

g_stage_trace <- ggplot(basic_stats, aes(x=stage, group=sample)) +
  theme_kit

# Plot reads over preprocessing
g_reads_stages <- g_stage_trace +
  geom_line(aes(y=n_read_pairs)) +
  scale_y_continuous("# Reads", expand=c(0,0), limits=c(0,NA))
g_reads_stages

# Plot relative read losses during preprocessing
g_reads_rel <- ggplot(n_reads_rel, 
                      aes(x=stage, group=sample)) +
  geom_line(aes(y=p_reads_lost_abs_marginal)) +
  scale_y_continuous("% Total Reads Lost", expand=c(0,0), 
                     labels = function(x) x*100) +
  theme_kit
g_reads_rel
```

TODO interpret the below plot.

```{r}
#| warning: false
#| label: plot-quality
#| fig-height: 8

g_qual <- ggplot(mapping=aes(linetype=read_pair, group=interaction(sample,read_pair))) + 
  scale_linetype_discrete(name = "Read") +
  theme_base

# Visualize quality
g_quality_base <- g_qual +
  geom_hline(yintercept=25, linetype="dashed", color="red") +
  geom_hline(yintercept=30, linetype="dashed", color="red") +
  geom_line(aes(x=position, y=mean_phred_score), data=quality_base_stats) +
  scale_y_continuous(name="Mean Phred score", expand=c(0,0), limits=c(10,45)) +
  scale_x_continuous(name="Position", limits=c(0,NA),
                     breaks=seq(0,140,20), expand=c(0,0)) +
  facet_grid(stage~.)
g_quality_base

g_quality_seq <- g_qual +
  geom_vline(xintercept=25, linetype="dashed", color="red") +
  geom_vline(xintercept=30, linetype="dashed", color="red") +
  geom_line(aes(x=mean_phred_score, y=n_sequences), data=quality_seq_stats) +
  scale_x_continuous(name="Mean Phred score", expand=c(0,0)) +
  scale_y_continuous(name="# Sequences", expand=c(0,0)) +
  facet_grid(stage~., scales = "free_y")
g_quality_seq
```

```{r}
#| label: preproc-dedup
#| fig-height: 4
#| fig-width: 6

stage_dup <- basic_stats %>% group_by(stage) %>% 
  summarize(dmin = min(percent_duplicates), dmax=max(percent_duplicates),
            dmean=mean(percent_duplicates), .groups = "drop")

g_dup_stages <- g_stage_trace +
  geom_line(aes(y=percent_duplicates)) +
  scale_y_continuous("% Duplicates", limits=c(0,NA), expand=c(0,0))
g_dup_stages

g_readlen_stages <- g_stage_trace + geom_line(aes(y=mean_seq_len)) +
  scale_y_continuous("Mean read length (nt)", expand=c(0,0), limits=c(0,NA))
g_readlen_stages
```

# Taxonomic composition

## High-level composition

To assess the high-level composition of the reads, I ran the ribodepleted files through Kraken2 and summarized the results with Bracken.

```{r}
#| label: prepare-composition
#| include: false

classifications <- c("Filtered", "Duplicate", "Ribosomal", "Unassigned",
                     "Bacterial", "Archaeal", "Viral", "Human")

# Import composition data
tax_final_dir <- file.path(results_dir, "new_taxonomy_final")
comp <- read_tsv(sprintf('%s/taxonomic_composition.tsv.gz', tax_final_dir)) %>% 
  left_join(libraries, by='sample') %>% drop_na()
comp_minor <- comp %>% 
  filter(classification %in% c("Archaeal", "Viral", "Human", "Other"))
comp_assigned <- comp %>%
  filter(! classification %in% c("Filtered", "Duplicate", 
                                 "Ribosomal", "Unassigned")) %>%
  group_by(sample) %>%
  mutate(p_reads = n_reads/sum(n_reads))
comp_assigned_minor <- comp_assigned %>% 
  filter(classification %in% c("Archaeal", "Viral", "Human", "Other"))

# Summarize composition
read_comp_summ <- comp %>% 
  group_by(classification) %>%
  summarize(n_reads = sum(n_reads), .groups = "drop_last") %>%
  mutate(n_reads = replace_na(n_reads,0),
         p_reads = n_reads/sum(n_reads),
         pc_reads = p_reads*100)

```

```{r}
#| label: plot-composition-all
#| fig-height: 7
#| fig-width: 25

# Prepare plotting templates
g_comp_base <- ggplot(mapping=aes(x=library, y=p_reads, fill=classification)) +
  scale_x_discrete(name="Plasma pool") +
  theme_kit + 
  theme(plot.title = element_text(hjust=0, face="plain", size=rel(1.5)))
scale_y_pc_reads <- purrr::partial(scale_y_continuous, name = "% of reads",
                                   expand = c(0,0), labels = function(y) y*100)
geom_comp <- purrr::partial(geom_col, position = "stack", width = 1)

# Plot overall composition
g_comp <- g_comp_base + geom_comp(data = comp) +
  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +
  scale_fill_brewer(palette = "Set1", name = "Classification") +
  ggtitle("Read composition (all reads, all groups)")
g_comp

# Repeat for classified reads only
palette_assigned <- brewer.pal(9, "Set1")[5:9]
g_comp_assigned <- g_comp_base + 
  geom_comp(data = comp_assigned) +
  scale_y_pc_reads(limits = c(0,1.01), breaks = seq(0,1,0.2)) +
  scale_fill_manual(values=palette_assigned, name = "Classification") +
  ggtitle("Read composition (assigned reads, all groups)")
g_comp_assigned

# Plot composition of minor components
palette_minor <- brewer.pal(9, "Set1")[6:9]
g_comp_minor <- g_comp_base + 
  geom_comp(data=comp_minor) +
  scale_y_pc_reads() +
  scale_fill_manual(values=palette_minor, name = "Classification") +
  ggtitle("Read composition (all reads, minor groups)")
g_comp_minor
g_comp_assigned_minor <- g_comp_base + 
  geom_comp(data=comp_assigned_minor) +
  scale_y_pc_reads() +
  scale_fill_manual(values=palette_minor, name = "Classification") +
  ggtitle("Read composition (assigned reads, minor groups)")
g_comp_assigned_minor
```

TODO analyze this above

## Total viral content

Total viral fraction average $6.13 \times 10^{-6}$ across samples. As a fraction of assigned (rather than total) reads, this jumped to $1.88 \times 10^{-3}$:

```{r}
#| label: p-viral
#| fig-width: 25
#| warning: false

p_reads_viral_all <- comp %>% filter(classification == "Viral") %>%
  mutate(read_group = "All reads")
p_reads_viral_assigned <- comp_assigned %>% filter(classification == "Viral") %>%
  mutate(read_group = "Classified reads")
p_reads_viral <- bind_rows(p_reads_viral_all, p_reads_viral_assigned)

# Plot
g_viral <- ggplot(p_reads_viral, aes(x=library, y=p_reads)) +
  geom_point() +
  scale_x_discrete(name="Plasma pool") +
  scale_y_log10(name="Viral read fraction") +
  facet_grid(.~read_group, scales = "free") +
  guides(color=guide_legend(nrow=2), shape=guide_legend(nrow=2),
         linetype=guide_legend(nrow=2)) +
  theme_kit
g_viral
```

## Taxonomic composition of viruses

When we drop down to the taxonomic composition of viral families, we go from 167 samples to 119 samples.

```{r}
#| label: extract-viral-taxa

# Get viral taxonomy
viral_taxa_path <- file.path(data_dir, "total-virus-db.tsv.gz")
viral_taxa <- read_tsv(viral_taxa_path, show_col_types = FALSE)

# Get Kraken reports
reports_path <- file.path(tax_final_dir, "kraken_reports.tsv.gz")
reports <- read_tsv(reports_path, show_col_types = FALSE) %>%
  inner_join(libraries, by="sample") %>% arrange(sample)

# Filter to viral taxa
kraken_reports_viral <- filter(reports, taxid %in% viral_taxa$taxid) %>%
  group_by(sample) %>%
  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])
kraken_reports_viral_cleaned <- kraken_reports_viral %>%
  select(-pc_reads_total, -n_reads_direct, -contains("minimizers")) %>%
  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %>% ungroup

viral_classes <- kraken_reports_viral_cleaned %>% filter(rank == "C")
viral_families <- kraken_reports_viral_cleaned %>% filter(rank == "F")

```

```{r}
#| label: viral-family-composition
#| fig-height: 4
#| fig-width: 25

major_threshold <- 0.01

# Identify major viral families
viral_families_major_tab <- viral_families %>% 
  group_by(name, taxid) %>%
  summarize(p_reads_viral_max = max(p_reads_viral), .groups="drop") %>%
  filter(p_reads_viral_max >= major_threshold)
viral_families_major_list <- viral_families_major_tab %>% pull(name)
viral_families_major <- viral_families %>% 
  filter(name %in% viral_families_major_list) %>%
  select(name, taxid, sample, p_reads_viral)
viral_families_minor <- viral_families_major %>% 
  group_by(sample) %>%
  summarize(p_reads_viral_major = sum(p_reads_viral), .groups = "drop") %>%
  mutate(name = "Other", taxid=NA, p_reads_viral = 1-p_reads_viral_major) %>%
  select(name, taxid, sample, p_reads_viral)
viral_families_display <- viral_families_major %>% 
  bind_rows(viral_families_minor) %>%
  arrange(desc(p_reads_viral)) %>% 
  mutate(name = factor(name, levels=c(viral_families_major_list, "Other"))) %>%
  rename(p_reads = p_reads_viral, classification=name) %>%
  inner_join(libraries, by='sample')

# Plot
palette_viral <- c(brewer.pal(12, "Set3"), brewer.pal(8, "Dark2"))
g_families <- g_comp_base + 
  geom_comp(data=viral_families_display) +
  scale_y_continuous(name="% Viral Reads", limits=c(0,1.01), 
                     breaks = seq(0,1,0.2),
                     expand=c(0,0), labels = function(y) y*100) +
  scale_fill_manual(values=palette_viral, name = "Viral class")
g_families
```

```{r}
#| label: viral-family-deep
#| fig-height: 4
#| fig-width: 8

# Get viral taxonomy
viral_taxa_path <- file.path(data_dir, "total-virus-db.tsv.gz")
viral_taxa <- read_tsv(viral_taxa_path, show_col_types = FALSE)

# Get Kraken reports
reports_path <- file.path(tax_final_dir, "kraken_reports.tsv.gz")
reports <- read_tsv(reports_path, show_col_types = FALSE) %>%
  inner_join(libraries, by="sample") %>% arrange(sample) %>% drop_na() 

# Filter to viral taxa
kraken_reports_viral <- filter(reports, taxid %in% viral_taxa$taxid) %>%
  group_by(sample) %>%
  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])
kraken_reports_viral_cleaned <- kraken_reports_viral %>%
  select(-pc_reads_total, -n_reads_direct, -contains("minimizers")) %>%
  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %>% 
  rename(old_rank=rank) %>% ungroup

major_threshold <- 0.01

# Identify major viral families
all_virus_name <- kraken_reports_viral_cleaned %>% 
  group_by(name, taxid) %>%
  summarize(p_reads_viral_max = max(p_reads_viral), .groups="drop") %>%
  filter(p_reads_viral_max >= major_threshold) %>%
  pull(name)

kraken_reports_viral_cleaned <- kraken_reports_viral_cleaned %>% 
  filter(name %in% all_virus_name) %>%
  select(name, taxid, sample, p_reads_viral, n_reads_clade)

```

```{r}
#| label: family_to_genus
#| fig-height: 4
#| fig-width: 8

#ranks_of_interest <- rev(c("species", "subgenus", "genus", "subfamily", "family"))
ranks_of_interest <- c("family", "genus")

# Filter the ranks we're interested in
filtered_taxa <- viral_taxa %>%
  filter(rank %in% ranks_of_interest)

# Join the datasets and create the links dataframe
links <- kraken_reports_viral_cleaned %>%
  inner_join(filtered_taxa, by = c("taxid", "name")) %>%
  inner_join(filtered_taxa, by = c("parent_taxid" = "taxid"), suffix = c("", "_parent")) %>%
  #filter(rank != rank_parent) %>%  # Ensure source and target are not in the same rank
  select(source = name_parent, target = name, value = p_reads_viral)

# Create nodes dataframe from unique names in links
nodes <- data.frame(
  name = c(links$source, links$target) %>% unique()
) %>%
  mutate(node = row_number() - 1)  # zero-indexed for networkD3

# Join rank information to nodes
nodes <- nodes %>%
  left_join(filtered_taxa, by = c("name")) %>%
  select(name, node, rank)

# Add IDs to links dataframe
links <- links %>%
  left_join(nodes, by = c("source" = "name")) %>%
  rename(IDsource = node) %>%
  left_join(nodes, by = c("target" = "name")) %>%
  rename(IDtarget = node)

# Aggregate link values
links_aggregated <- links %>%
  group_by(IDsource, IDtarget, source, target) %>%
  summarise(value = sum(value), .groups = "drop")

# Get rid of some warnings
links_aggregated <- as.data.frame(links_aggregated)
nodes <- as.data.frame(nodes)

# Create Sankey diagram
sankey_diagram <- sankeyNetwork(Links = links_aggregated, 
                                Nodes = nodes,
                                Source = "IDsource", 
                                Target = "IDtarget",
                                Value = "value", 
                                NodeID = "name",
                                fontSize = 12,
                                NodeGroup = "rank",
                                nodeWidth = 30,
                                height = 800,
                                width = 1000,
                                sinksRight = FALSE)

# Display the diagram
#sankey_diagram
```

## Number of reads for viruses of interest

```{r}
#| label: extract-viral-taxa-two
#| include: false

# Get viral taxonomy
viral_taxa_path <- file.path(data_dir, "total-virus-db.tsv.gz")
viral_taxa <- read_tsv(viral_taxa_path, show_col_types = FALSE)

# Get Kraken reports
reports_path <- file.path(tax_final_dir, "kraken_reports.tsv.gz")
reports <- read_tsv(reports_path, show_col_types = FALSE) %>%
  inner_join(libraries, by="sample") %>% arrange(sample) %>% drop_na()

# Filter to viral taxa
kraken_reports_viral <- filter(reports, taxid %in% viral_taxa$taxid) %>%
  group_by(sample) %>%
  mutate(p_reads_viral = n_reads_clade/n_reads_clade[1])
kraken_reports_viral_cleaned <- kraken_reports_viral %>%
  select(-pc_reads_total, -n_reads_direct, -contains("minimizers")) %>%
  select(name, taxid, p_reads_viral, n_reads_clade, everything()) %>% ungroup

viral_classes <- kraken_reports_viral_cleaned %>% filter(rank == "C")
viral_families <- kraken_reports_viral_cleaned %>% filter(rank == "F")
viral_genus <- kraken_reports_viral_cleaned %>% filter(rank == "G")
viral_species <- kraken_reports_viral_cleaned %>% filter(rank == "S")
```

We're particulary interested in the following viruses:

1.  Hepatitis B
    -   Family: Hepadnaviridae
    -   Genus: Orthohepadnavirus
    -   Species: Hepatitis B virus
2.  Hepatitis C
    -   Family: Flaviviridae
    -   Genus: Hepacivirus
    -   Species: Hepatitis C virus
3.  EBV (Epstein-Barr virus)
    -   Family: Herpesviridae
    -   Genus: Lymphocryptovirus
    -   Species: Human gammaherpesvirus 4
4.  CMV (Cytomegalovirus)
    -   Family: Herpesviridae
    -   Genus: Cytomegalovirus
    -   Species: Human betaherpesvirus 5
5.  HIV-1 (Human Immunodeficiency Virus type 1)
    -   Family: Retroviridae
    -   Genus: Lentivirus
    -   Species: Human immunodeficiency virus 1
6.  Anelloviridae
    -   Family: Anelloviridae

```{r}
#| label: family-detection

#reads_post_process <- basic_stats %>% filter(stage == 'ribo_secondary') %>% 
  #select(sample, n_read_pairs)

viral_families_interest <- viral_families %>% filter(name == "Hepadnaviridae" | 
    name ==  "Flaviviridae" | 
    name ==  "Orthoherpesviridae" |
    name ==  "Herpesviridae" |
    name ==  "Retroviridae" |
    name == "Anelloviridae") %>% group_by(name) %>% summarize(sum=sum(n_reads_clade))

viral_families_interest

```

```{r}
#| label: genus-detection

# Species
# Hepatitis B virus <- 10407
# Hepatitis C virus <- 10407
# HIV - 1 <- 11676

viral_families_interest <- viral_genus %>% filter( name == "Orthohepadnavirus" | 
    name ==  "Hepacivirus" | 
    name ==  "Lymphocryptovirus" |
    name ==  "Cytomegalovirus" |
    name == "Lentivirus") %>% group_by(name) %>% summarize(sum=sum(n_reads_clade))

viral_families_interest

```

# Conclusion

Second rough analysis, I plan on going in-depth during the next few days, luckily this revealed a few bugs that I hope to take care off.

After going through this a second time, I realized that there are three potential bugs: - Forgot to remove prefix of stage from sample names in multiqc_single_format - (may be connected to above) adapter statistics are not calculated - Getting negative unassigned reads from kraken, I assume this probably has to do with some paired stuff
